{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7f21a49",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c3e51cc-95ad-4a58-bbf4-7cc2bd1da83c",
   "metadata": {},
   "source": [
    "#### Best Params\n",
    ">  **Obtained Via Hyper Parameter Optimization**\n",
    "\n",
    ">\n",
    ">    **For RELIANCE.NS only  Need to perform Hyper-Parameter Optimization for other Stocks.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9ae8d0-d770-40a8-9ab0-daa0a3df13c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Hyper-Paramameter Values for RELIANCE.NS Stock.\n",
    "best_params = {\n",
    "    'hidden_size': 765,\n",
    "    'num_layers': 1,\n",
    "    'dropout': 0.09706022396783648,\n",
    "    'learning_rate': 0.0014995936033621617,\n",
    "    'weight_decay': 0.0009768589023152823,\n",
    "    'batch_norm': False,\n",
    "    'cell_dropout': 0.04166033473442454,\n",
    "    'optimizer': 'adamw',\n",
    "    'lr_scheduler': 'cosine',\n",
    "    'lr_T_max': 49,\n",
    "    'loss_function': 'huber',\n",
    "    'delta': 0.1403443377256188\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0c8d45-c5a3-493d-969a-120839816798",
   "metadata": {},
   "source": [
    "####  1. Select Stock Ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96720a6b-e4c8-4480-85ad-61438be2cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set the ticker symbol\n",
    "ticker = \"INFY.NS\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4a2103-01ea-49a4-a4e5-5303e5530c15",
   "metadata": {},
   "source": [
    "#### 2. Select appropriate device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816bb42a-e447-4405-963f-e4ee5f83e50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Select appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a47ee33-eef3-45f2-bff1-ac4a61514a41",
   "metadata": {},
   "source": [
    "#### 3. Run the CUDA check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12c5d306-f730-439b-9aeb-41e36512fbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available with 1 device(s).\n",
      "Current device: 0 - NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "CUDA version: 12.1\n",
      "Total GPU memory     : 4.29444301 GB\n",
      "Currently allocated  : 0.00000000 GB\n",
      "Currently reserved   : 0.00000000 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Config.check_cuda_config import check_cuda_configuration\n",
    "\n",
    "# Run the CUDA check\n",
    "check_cuda_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cb156e-1d9f-4a1b-ae36-95109d94b189",
   "metadata": {},
   "source": [
    "#### 4. Fetch stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2c3bbb2-3af3-477a-8929-2e5cb3d379e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched raw data saved to: Data/RawData\\INFY.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>232.496413</td>\n",
       "      <td>234.277995</td>\n",
       "      <td>231.186955</td>\n",
       "      <td>232.728027</td>\n",
       "      <td>4069264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>233.387150</td>\n",
       "      <td>234.340292</td>\n",
       "      <td>231.614486</td>\n",
       "      <td>233.507416</td>\n",
       "      <td>6895528</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>238.999118</td>\n",
       "      <td>238.999118</td>\n",
       "      <td>229.387496</td>\n",
       "      <td>230.100128</td>\n",
       "      <td>6817288</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>230.411960</td>\n",
       "      <td>230.411960</td>\n",
       "      <td>224.318938</td>\n",
       "      <td>224.929138</td>\n",
       "      <td>10892600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>224.390221</td>\n",
       "      <td>224.773265</td>\n",
       "      <td>218.956396</td>\n",
       "      <td>219.508682</td>\n",
       "      <td>12649312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close    Volume  \\\n",
       "2010-01-04  232.496413  234.277995  231.186955  232.728027   4069264   \n",
       "2010-01-05  233.387150  234.340292  231.614486  233.507416   6895528   \n",
       "2010-01-06  238.999118  238.999118  229.387496  230.100128   6817288   \n",
       "2010-01-07  230.411960  230.411960  224.318938  224.929138  10892600   \n",
       "2010-01-08  224.390221  224.773265  218.956396  219.508682  12649312   \n",
       "\n",
       "            Dividends  Stock Splits  \n",
       "2010-01-04        0.0           0.0  \n",
       "2010-01-05        0.0           0.0  \n",
       "2010-01-06        0.0           0.0  \n",
       "2010-01-07        0.0           0.0  \n",
       "2010-01-08        0.0           0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from DataPipeline.data_fetcher import fetch_daily_data_ist\n",
    "\n",
    "# Create the path if it doesn't exist\n",
    "raw_data_dir = \"Data/RawData\"\n",
    "\n",
    "# Fetch stock data\n",
    "data = fetch_daily_data_ist(ticker)\n",
    "\n",
    "# Construct filename and full path\n",
    "filename = f\"{ticker[:-3]}.csv\"\n",
    "filepath = os.path.join(raw_data_dir, filename)\n",
    "\n",
    "# Save to CSV\n",
    "data.to_csv(filepath)\n",
    "print(f\"Fetched raw data saved to: {filepath}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb345f3-45aa-4f53-9a3d-aba0096e8fce",
   "metadata": {},
   "source": [
    "#### 5. Calculate Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63c463d7-703b-4952-a89e-72a7fcfb1062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataPipeline.technical_indicators import TechnicalIndicators\n",
    "\n",
    "# Calculate technical indicators\n",
    "indicators = TechnicalIndicators(data)\n",
    "indicators_data = indicators.calculate_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5232e794-c0ff-4b23-bd44-3b9f325ebe65",
   "metadata": {},
   "source": [
    "#### 6. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87703c18-5e35-4d98-9ffb-5ef3eb597394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed data shape after indicators: (3802, 81)\n",
      "\n",
      "Data split sizes:\n",
      "\n",
      "Train: 3231, Test: 571\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nProcessed data shape after indicators: {indicators_data.shape}\")\n",
    "\n",
    "# Split data into train and validation (85%) and test (15%)\n",
    "total_size = len(indicators_data)\n",
    "train_size = int(0.85 * total_size)\n",
    "\n",
    "train_data = indicators_data[:train_size].copy()\n",
    "test_data = indicators_data[train_size:].copy()\n",
    "\n",
    "print(f\"\\nData split sizes:\")\n",
    "print(f\"\\nTrain: {len(train_data)}, Test: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39821baf-ab1c-4f82-89a4-9f6a8a744bed",
   "metadata": {},
   "source": [
    "#### 7. Dropping NaN Columns and Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e72d01b-aabd-47d2-aebe-88119f5a4fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No columns with all null values found.\n",
      "No columns with all null values found.\n",
      "\n",
      "Dropped columns due to nulls: []\n",
      "Final shapes - Train: (3204, 81), Test: (570, 81)\n"
     ]
    }
   ],
   "source": [
    "from DataPipeline.data_cleaning import drop_all_null_columns\n",
    "\n",
    "# Drop all-null columns\n",
    "train_data_ready, dropped_columns = drop_all_null_columns(train_data)\n",
    "test_data_ready, _ = drop_all_null_columns(test_data)\n",
    "\n",
    "print(f\"\\nDropped columns due to nulls: {dropped_columns}\")\n",
    "print(f\"Final shapes - Train: {train_data_ready.shape}, Test: {test_data_ready.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b08eca9-e706-4fc3-943a-cb771290ce88",
   "metadata": {},
   "source": [
    "#### 8. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fc83a6a-426d-429b-8db6-7c5e34d0dd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Optimization.prepare_data_for_hpo import prepare_data_for_hpo\n",
    "\n",
    "# Prepare data for LSTM (only train loader, no val)\n",
    "data, train_loader, val_loader, X_scaler, y_scaler, feature_names, num_features = prepare_data_for_hpo(\n",
    "    Modelling_data=train_data_ready,\n",
    "    model_type=\"LSTM\",\n",
    "    batch_size=64,\n",
    "    seq_length=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e08e064-74c5-4171-9af8-4110adf93a7e",
   "metadata": {},
   "source": [
    "#### 9. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "557d1260-a1fa-431e-8497-18a3bd2e5d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training final model with best hyperparameters...\n",
      " {'hidden_size': 765, 'num_layers': 1, 'dropout': 0.09706022396783648, 'learning_rate': 0.0014995936033621617, 'weight_decay': 0.0009768589023152823, 'batch_norm': False, 'cell_dropout': 0.04166033473442454, 'optimizer': 'adamw', 'lr_scheduler': 'cosine', 'lr_T_max': 49, 'loss_function': 'huber', 'delta': 0.1403443377256188}\n",
      "Epoch 1/500 - Train Loss: 0.02977309, Val Loss: 0.16111812 (Best Model Saved)\n",
      "Epoch 2/500 - Train Loss: 0.00489675, Val Loss: 0.04369849 (Best Model Saved)\n",
      "Epoch 3/500 - Train Loss: 0.00131868, Val Loss: 0.07206188\n",
      "Epoch 4/500 - Train Loss: 0.00192032, Val Loss: 0.01996631 (Best Model Saved)\n",
      "Epoch 5/500 - Train Loss: 0.00108112, Val Loss: 0.03384035\n",
      "Epoch 6/500 - Train Loss: 0.00096074, Val Loss: 0.00855320 (Best Model Saved)\n",
      "Epoch 7/500 - Train Loss: 0.00077805, Val Loss: 0.02292035\n",
      "Epoch 8/500 - Train Loss: 0.00160311, Val Loss: 0.04674455\n",
      "Epoch 9/500 - Train Loss: 0.00054935, Val Loss: 0.00641991 (Best Model Saved)\n",
      "Epoch 10/500 - Train Loss: 0.00045987, Val Loss: 0.02086773\n",
      "Epoch 11/500 - Train Loss: 0.00050898, Val Loss: 0.01150149\n",
      "Epoch 12/500 - Train Loss: 0.00146167, Val Loss: 0.02887191\n",
      "Epoch 13/500 - Train Loss: 0.00038146, Val Loss: 0.00692916\n",
      "Epoch 14/500 - Train Loss: 0.00058899, Val Loss: 0.01455618\n",
      "Epoch 15/500 - Train Loss: 0.00041828, Val Loss: 0.00590284 (Best Model Saved)\n",
      "Epoch 16/500 - Train Loss: 0.00026246, Val Loss: 0.00565572 (Best Model Saved)\n",
      "Epoch 17/500 - Train Loss: 0.00041502, Val Loss: 0.00396091 (Best Model Saved)\n",
      "Epoch 18/500 - Train Loss: 0.00036297, Val Loss: 0.00863418\n",
      "Epoch 19/500 - Train Loss: 0.00024377, Val Loss: 0.00347114 (Best Model Saved)\n",
      "Epoch 20/500 - Train Loss: 0.00024123, Val Loss: 0.00393857\n",
      "Epoch 21/500 - Train Loss: 0.00020997, Val Loss: 0.00247822 (Best Model Saved)\n",
      "Epoch 22/500 - Train Loss: 0.00027213, Val Loss: 0.00560065\n",
      "Epoch 23/500 - Train Loss: 0.00038625, Val Loss: 0.00462078\n",
      "Epoch 24/500 - Train Loss: 0.00067902, Val Loss: 0.00537387\n",
      "Epoch 25/500 - Train Loss: 0.00020450, Val Loss: 0.00643692\n",
      "Epoch 26/500 - Train Loss: 0.00018641, Val Loss: 0.01034523\n",
      "Epoch 27/500 - Train Loss: 0.00028678, Val Loss: 0.00365111\n",
      "Epoch 28/500 - Train Loss: 0.00039079, Val Loss: 0.00782604\n",
      "Epoch 29/500 - Train Loss: 0.00016297, Val Loss: 0.00412117\n",
      "Epoch 30/500 - Train Loss: 0.00014614, Val Loss: 0.01007917\n",
      "Epoch 31/500 - Train Loss: 0.00022410, Val Loss: 0.00711281\n",
      "Epoch 32/500 - Train Loss: 0.00014322, Val Loss: 0.00284072\n",
      "Epoch 33/500 - Train Loss: 0.00014855, Val Loss: 0.00174400 (Best Model Saved)\n",
      "Epoch 34/500 - Train Loss: 0.00011910, Val Loss: 0.00174768\n",
      "Epoch 35/500 - Train Loss: 0.00015701, Val Loss: 0.00864103\n",
      "Epoch 36/500 - Train Loss: 0.00014836, Val Loss: 0.00235227\n",
      "Epoch 37/500 - Train Loss: 0.00012132, Val Loss: 0.00376615\n",
      "Epoch 38/500 - Train Loss: 0.00011019, Val Loss: 0.00446625\n",
      "Epoch 39/500 - Train Loss: 0.00011612, Val Loss: 0.00198595\n",
      "Epoch 40/500 - Train Loss: 0.00010944, Val Loss: 0.00132737 (Best Model Saved)\n",
      "Epoch 41/500 - Train Loss: 0.00011335, Val Loss: 0.00115627 (Best Model Saved)\n",
      "Epoch 42/500 - Train Loss: 0.00009751, Val Loss: 0.00134024\n",
      "Epoch 43/500 - Train Loss: 0.00009086, Val Loss: 0.00165203\n",
      "Epoch 44/500 - Train Loss: 0.00009476, Val Loss: 0.00191569\n",
      "Epoch 45/500 - Train Loss: 0.00008925, Val Loss: 0.00198676\n",
      "Epoch 46/500 - Train Loss: 0.00010050, Val Loss: 0.00140807\n",
      "Epoch 47/500 - Train Loss: 0.00010451, Val Loss: 0.00124165\n",
      "Epoch 48/500 - Train Loss: 0.00010128, Val Loss: 0.00144710\n",
      "Epoch 49/500 - Train Loss: 0.00010180, Val Loss: 0.00145860\n",
      "Epoch 50/500 - Train Loss: 0.00009856, Val Loss: 0.00145860\n",
      "Epoch 51/500 - Train Loss: 0.00010703, Val Loss: 0.00152460\n",
      "Epoch 52/500 - Train Loss: 0.00008964, Val Loss: 0.00147622\n",
      "Epoch 53/500 - Train Loss: 0.00009545, Val Loss: 0.00133595\n",
      "Epoch 54/500 - Train Loss: 0.00011048, Val Loss: 0.00262128\n",
      "Epoch 55/500 - Train Loss: 0.00012349, Val Loss: 0.00126425\n",
      "Epoch 56/500 - Train Loss: 0.00009360, Val Loss: 0.00145979\n",
      "Epoch 57/500 - Train Loss: 0.00009876, Val Loss: 0.00130742\n",
      "Epoch 58/500 - Train Loss: 0.00009812, Val Loss: 0.00139809\n",
      "Epoch 59/500 - Train Loss: 0.00009433, Val Loss: 0.00238622\n",
      "Epoch 60/500 - Train Loss: 0.00011332, Val Loss: 0.00315479\n",
      "Epoch 61/500 - Train Loss: 0.00012024, Val Loss: 0.00216787\n",
      "Epoch 62/500 - Train Loss: 0.00010786, Val Loss: 0.00120757\n",
      "Epoch 63/500 - Train Loss: 0.00009956, Val Loss: 0.00374561\n",
      "Epoch 64/500 - Train Loss: 0.00013427, Val Loss: 0.00616194\n",
      "Epoch 65/500 - Train Loss: 0.00016534, Val Loss: 0.00419855\n",
      "Epoch 66/500 - Train Loss: 0.00018503, Val Loss: 0.00152317\n",
      "Epoch 67/500 - Train Loss: 0.00018589, Val Loss: 0.01628789\n",
      "Epoch 68/500 - Train Loss: 0.00022932, Val Loss: 0.01413010\n",
      "Epoch 69/500 - Train Loss: 0.00032045, Val Loss: 0.00216413\n",
      "Epoch 70/500 - Train Loss: 0.00018470, Val Loss: 0.00535502\n",
      "Epoch 71/500 - Train Loss: 0.00026655, Val Loss: 0.00273877\n",
      "Epoch 72/500 - Train Loss: 0.00033201, Val Loss: 0.01703890\n",
      "Epoch 73/500 - Train Loss: 0.00032669, Val Loss: 0.01257432\n",
      "Epoch 74/500 - Train Loss: 0.00035572, Val Loss: 0.01473852\n",
      "Epoch 75/500 - Train Loss: 0.00046148, Val Loss: 0.00636553\n",
      "Epoch 76/500 - Train Loss: 0.00023860, Val Loss: 0.00815967\n",
      "Epoch 77/500 - Train Loss: 0.00038060, Val Loss: 0.01388820\n",
      "Epoch 78/500 - Train Loss: 0.00025047, Val Loss: 0.03838875\n",
      "Epoch 79/500 - Train Loss: 0.00110467, Val Loss: 0.05740222\n",
      "Epoch 80/500 - Train Loss: 0.00048570, Val Loss: 0.00857094\n",
      "Epoch 81/500 - Train Loss: 0.00019427, Val Loss: 0.00455556\n",
      "Epoch 82/500 - Train Loss: 0.00078731, Val Loss: 0.04206225\n",
      "Epoch 83/500 - Train Loss: 0.00042741, Val Loss: 0.00240088\n",
      "Epoch 84/500 - Train Loss: 0.00031861, Val Loss: 0.00501352\n",
      "Epoch 85/500 - Train Loss: 0.00045063, Val Loss: 0.00268378\n",
      "Epoch 86/500 - Train Loss: 0.00045828, Val Loss: 0.00290774\n",
      "Epoch 87/500 - Train Loss: 0.00030278, Val Loss: 0.00163799\n",
      "Epoch 88/500 - Train Loss: 0.00025105, Val Loss: 0.00223629\n",
      "Epoch 89/500 - Train Loss: 0.00016460, Val Loss: 0.00162107\n",
      "Epoch 90/500 - Train Loss: 0.00043481, Val Loss: 0.12981853\n",
      "Epoch 91/500 - Train Loss: 0.00350062, Val Loss: 0.00890809\n",
      "Epoch 92/500 - Train Loss: 0.00035578, Val Loss: 0.01000752\n",
      "Epoch 93/500 - Train Loss: 0.00051874, Val Loss: 0.03266652\n",
      "Epoch 94/500 - Train Loss: 0.00045417, Val Loss: 0.02044156\n",
      "Epoch 95/500 - Train Loss: 0.00035844, Val Loss: 0.00310671\n",
      "Epoch 96/500 - Train Loss: 0.00023095, Val Loss: 0.00541226\n",
      "Epoch 97/500 - Train Loss: 0.00022532, Val Loss: 0.00790690\n",
      "Epoch 98/500 - Train Loss: 0.00026211, Val Loss: 0.02728445\n",
      "Epoch 99/500 - Train Loss: 0.00023301, Val Loss: 0.00419192\n",
      "Epoch 100/500 - Train Loss: 0.00051070, Val Loss: 0.00395399\n",
      "Epoch 101/500 - Train Loss: 0.00019411, Val Loss: 0.00411929\n",
      "Epoch 102/500 - Train Loss: 0.00018085, Val Loss: 0.00198659\n",
      "Epoch 103/500 - Train Loss: 0.00030671, Val Loss: 0.06337215\n",
      "Epoch 104/500 - Train Loss: 0.00204905, Val Loss: 0.02427814\n",
      "Epoch 105/500 - Train Loss: 0.00028606, Val Loss: 0.00701433\n",
      "Epoch 106/500 - Train Loss: 0.00015765, Val Loss: 0.01054433\n",
      "Epoch 107/500 - Train Loss: 0.00040090, Val Loss: 0.00248730\n",
      "Epoch 108/500 - Train Loss: 0.00016554, Val Loss: 0.01293302\n",
      "Epoch 109/500 - Train Loss: 0.00017840, Val Loss: 0.00221418\n",
      "Epoch 110/500 - Train Loss: 0.00023671, Val Loss: 0.00193685\n",
      "Epoch 111/500 - Train Loss: 0.00021808, Val Loss: 0.01036896\n",
      "Epoch 112/500 - Train Loss: 0.00023277, Val Loss: 0.00121481\n",
      "Epoch 113/500 - Train Loss: 0.00025133, Val Loss: 0.00124064\n",
      "Epoch 114/500 - Train Loss: 0.00017635, Val Loss: 0.00338984\n",
      "Epoch 115/500 - Train Loss: 0.00015816, Val Loss: 0.00147482\n",
      "Epoch 116/500 - Train Loss: 0.00012623, Val Loss: 0.00262086\n",
      "Epoch 117/500 - Train Loss: 0.00013687, Val Loss: 0.00205136\n",
      "Epoch 118/500 - Train Loss: 0.00012019, Val Loss: 0.00355839\n",
      "Epoch 119/500 - Train Loss: 0.00014260, Val Loss: 0.00374705\n",
      "Epoch 120/500 - Train Loss: 0.00012322, Val Loss: 0.00860681\n",
      "Epoch 121/500 - Train Loss: 0.00014653, Val Loss: 0.00311086\n",
      "Epoch 122/500 - Train Loss: 0.00012666, Val Loss: 0.00100684 (Best Model Saved)\n",
      "Epoch 123/500 - Train Loss: 0.00010905, Val Loss: 0.00218225\n",
      "Epoch 124/500 - Train Loss: 0.00009388, Val Loss: 0.00096932 (Best Model Saved)\n",
      "Epoch 125/500 - Train Loss: 0.00010587, Val Loss: 0.00143296\n",
      "Epoch 126/500 - Train Loss: 0.00011463, Val Loss: 0.00116107\n",
      "Epoch 127/500 - Train Loss: 0.00015680, Val Loss: 0.00195253\n",
      "Epoch 128/500 - Train Loss: 0.00016948, Val Loss: 0.00268881\n",
      "Epoch 129/500 - Train Loss: 0.00011966, Val Loss: 0.00321386\n",
      "Epoch 130/500 - Train Loss: 0.00009476, Val Loss: 0.00092596 (Best Model Saved)\n",
      "Epoch 131/500 - Train Loss: 0.00008824, Val Loss: 0.00116928\n",
      "Epoch 132/500 - Train Loss: 0.00008407, Val Loss: 0.00084656 (Best Model Saved)\n",
      "Epoch 133/500 - Train Loss: 0.00008408, Val Loss: 0.00224786\n",
      "Epoch 134/500 - Train Loss: 0.00008829, Val Loss: 0.00096690\n",
      "Epoch 135/500 - Train Loss: 0.00008224, Val Loss: 0.00124669\n",
      "Epoch 136/500 - Train Loss: 0.00007627, Val Loss: 0.00060118 (Best Model Saved)\n",
      "Epoch 137/500 - Train Loss: 0.00007657, Val Loss: 0.00219426\n",
      "Epoch 138/500 - Train Loss: 0.00007708, Val Loss: 0.00058912 (Best Model Saved)\n",
      "Epoch 139/500 - Train Loss: 0.00006844, Val Loss: 0.00048896 (Best Model Saved)\n",
      "Epoch 140/500 - Train Loss: 0.00007743, Val Loss: 0.00132974\n",
      "Epoch 141/500 - Train Loss: 0.00006828, Val Loss: 0.00077788\n",
      "Epoch 142/500 - Train Loss: 0.00006981, Val Loss: 0.00100969\n",
      "Epoch 143/500 - Train Loss: 0.00007158, Val Loss: 0.00065037\n",
      "Epoch 144/500 - Train Loss: 0.00006869, Val Loss: 0.00077133\n",
      "Epoch 145/500 - Train Loss: 0.00006274, Val Loss: 0.00066385\n",
      "Epoch 146/500 - Train Loss: 0.00007100, Val Loss: 0.00068267\n",
      "Epoch 147/500 - Train Loss: 0.00007257, Val Loss: 0.00069430\n",
      "Epoch 148/500 - Train Loss: 0.00006936, Val Loss: 0.00069430\n",
      "Epoch 149/500 - Train Loss: 0.00007602, Val Loss: 0.00071054\n",
      "Epoch 150/500 - Train Loss: 0.00007135, Val Loss: 0.00077477\n",
      "Epoch 151/500 - Train Loss: 0.00007272, Val Loss: 0.00100684\n",
      "Epoch 152/500 - Train Loss: 0.00007112, Val Loss: 0.00065399\n",
      "Epoch 153/500 - Train Loss: 0.00006533, Val Loss: 0.00105564\n",
      "Epoch 154/500 - Train Loss: 0.00007390, Val Loss: 0.00130372\n",
      "Epoch 155/500 - Train Loss: 0.00008168, Val Loss: 0.00083973\n",
      "Epoch 156/500 - Train Loss: 0.00009287, Val Loss: 0.00092533\n",
      "Epoch 157/500 - Train Loss: 0.00006842, Val Loss: 0.00084908\n",
      "Epoch 158/500 - Train Loss: 0.00006709, Val Loss: 0.00131168\n",
      "Epoch 159/500 - Train Loss: 0.00006971, Val Loss: 0.00404416\n",
      "Epoch 160/500 - Train Loss: 0.00007195, Val Loss: 0.00108599\n",
      "Epoch 161/500 - Train Loss: 0.00009466, Val Loss: 0.00128320\n",
      "Epoch 162/500 - Train Loss: 0.00008285, Val Loss: 0.00432359\n",
      "Epoch 163/500 - Train Loss: 0.00008767, Val Loss: 0.00075024\n",
      "Epoch 164/500 - Train Loss: 0.00012970, Val Loss: 0.00142431\n",
      "Epoch 165/500 - Train Loss: 0.00013782, Val Loss: 0.00301819\n",
      "Epoch 166/500 - Train Loss: 0.00009019, Val Loss: 0.00661246\n",
      "Epoch 167/500 - Train Loss: 0.00012144, Val Loss: 0.00096412\n",
      "Epoch 168/500 - Train Loss: 0.00013873, Val Loss: 0.01592770\n",
      "Epoch 169/500 - Train Loss: 0.00056713, Val Loss: 0.00288627\n",
      "Epoch 170/500 - Train Loss: 0.00014969, Val Loss: 0.00197480\n",
      "Epoch 171/500 - Train Loss: 0.00015531, Val Loss: 0.00533787\n",
      "Epoch 172/500 - Train Loss: 0.00016731, Val Loss: 0.00186647\n",
      "Epoch 173/500 - Train Loss: 0.00014982, Val Loss: 0.00340444\n",
      "Epoch 174/500 - Train Loss: 0.00015655, Val Loss: 0.00827541\n",
      "Epoch 175/500 - Train Loss: 0.00026302, Val Loss: 0.00196003\n",
      "Epoch 176/500 - Train Loss: 0.00021263, Val Loss: 0.05146545\n",
      "Epoch 177/500 - Train Loss: 0.00127769, Val Loss: 0.00270848\n",
      "Epoch 178/500 - Train Loss: 0.00042416, Val Loss: 0.00829583\n",
      "Epoch 179/500 - Train Loss: 0.00030744, Val Loss: 0.01112955\n",
      "Epoch 180/500 - Train Loss: 0.00015308, Val Loss: 0.00437786\n",
      "Epoch 181/500 - Train Loss: 0.00018743, Val Loss: 0.00165924\n",
      "Epoch 182/500 - Train Loss: 0.00027005, Val Loss: 0.01214634\n",
      "Epoch 183/500 - Train Loss: 0.00018502, Val Loss: 0.00225231\n",
      "Epoch 184/500 - Train Loss: 0.00036815, Val Loss: 0.01227134\n",
      "Epoch 185/500 - Train Loss: 0.00027713, Val Loss: 0.00504333\n",
      "Epoch 186/500 - Train Loss: 0.00027663, Val Loss: 0.00527176\n",
      "Epoch 187/500 - Train Loss: 0.00030428, Val Loss: 0.02363136\n",
      "Epoch 188/500 - Train Loss: 0.00023421, Val Loss: 0.00192853\n",
      "Epoch 189/500 - Train Loss: 0.00020091, Val Loss: 0.00664320\n",
      "Epoch 190/500 - Train Loss: 0.00021776, Val Loss: 0.02817651\n",
      "Epoch 191/500 - Train Loss: 0.00064804, Val Loss: 0.01973712\n",
      "Epoch 192/500 - Train Loss: 0.00094621, Val Loss: 0.02108032\n",
      "Epoch 193/500 - Train Loss: 0.00028954, Val Loss: 0.01212489\n",
      "Epoch 194/500 - Train Loss: 0.00030453, Val Loss: 0.00175501\n",
      "Epoch 195/500 - Train Loss: 0.00024225, Val Loss: 0.03219903\n",
      "Epoch 196/500 - Train Loss: 0.00030570, Val Loss: 0.00620074\n",
      "Epoch 197/500 - Train Loss: 0.00017844, Val Loss: 0.00332450\n",
      "Epoch 198/500 - Train Loss: 0.00022743, Val Loss: 0.00190563\n",
      "Epoch 199/500 - Train Loss: 0.00027416, Val Loss: 0.00821895\n",
      "Epoch 200/500 - Train Loss: 0.00015736, Val Loss: 0.01661659\n",
      "Epoch 201/500 - Train Loss: 0.00065145, Val Loss: 0.00268220\n",
      "Epoch 202/500 - Train Loss: 0.00024229, Val Loss: 0.01552791\n",
      "Epoch 203/500 - Train Loss: 0.00015480, Val Loss: 0.00157781\n",
      "Epoch 204/500 - Train Loss: 0.00019318, Val Loss: 0.01202890\n",
      "Epoch 205/500 - Train Loss: 0.00019589, Val Loss: 0.00198370\n",
      "Epoch 206/500 - Train Loss: 0.00019317, Val Loss: 0.00300506\n",
      "Epoch 207/500 - Train Loss: 0.00017719, Val Loss: 0.00718175\n",
      "Epoch 208/500 - Train Loss: 0.00028895, Val Loss: 0.00165370\n",
      "Epoch 209/500 - Train Loss: 0.00017190, Val Loss: 0.01365033\n",
      "Epoch 210/500 - Train Loss: 0.00017448, Val Loss: 0.00126012\n",
      "Epoch 211/500 - Train Loss: 0.00021134, Val Loss: 0.00101081\n",
      "Epoch 212/500 - Train Loss: 0.00011875, Val Loss: 0.00167400\n",
      "Epoch 213/500 - Train Loss: 0.00017764, Val Loss: 0.00298843\n",
      "Epoch 214/500 - Train Loss: 0.00021139, Val Loss: 0.00333369\n",
      "Epoch 215/500 - Train Loss: 0.00011850, Val Loss: 0.00468111\n",
      "Epoch 216/500 - Train Loss: 0.00018437, Val Loss: 0.00203792\n",
      "Epoch 217/500 - Train Loss: 0.00012997, Val Loss: 0.01342302\n",
      "Epoch 218/500 - Train Loss: 0.00023716, Val Loss: 0.00389220\n",
      "Epoch 219/500 - Train Loss: 0.00010045, Val Loss: 0.00153923\n",
      "Epoch 220/500 - Train Loss: 0.00018262, Val Loss: 0.00126485\n",
      "Epoch 221/500 - Train Loss: 0.00009830, Val Loss: 0.00327460\n",
      "Epoch 222/500 - Train Loss: 0.00010009, Val Loss: 0.00123470\n",
      "Epoch 223/500 - Train Loss: 0.00012790, Val Loss: 0.00163957\n",
      "Epoch 224/500 - Train Loss: 0.00010828, Val Loss: 0.00250480\n",
      "Epoch 225/500 - Train Loss: 0.00010518, Val Loss: 0.00539836\n",
      "Epoch 226/500 - Train Loss: 0.00010361, Val Loss: 0.00085062\n",
      "Epoch 227/500 - Train Loss: 0.00008483, Val Loss: 0.00480869\n",
      "Epoch 228/500 - Train Loss: 0.00011128, Val Loss: 0.00094215\n",
      "Epoch 229/500 - Train Loss: 0.00011074, Val Loss: 0.00353738\n",
      "Epoch 230/500 - Train Loss: 0.00015675, Val Loss: 0.00121415\n",
      "Epoch 231/500 - Train Loss: 0.00015130, Val Loss: 0.00335014\n",
      "Epoch 232/500 - Train Loss: 0.00009476, Val Loss: 0.00305618\n",
      "Epoch 233/500 - Train Loss: 0.00010862, Val Loss: 0.00140702\n",
      "Epoch 234/500 - Train Loss: 0.00007652, Val Loss: 0.00276436\n",
      "Epoch 235/500 - Train Loss: 0.00007779, Val Loss: 0.00126585\n",
      "Epoch 236/500 - Train Loss: 0.00007362, Val Loss: 0.00177906\n",
      "Epoch 237/500 - Train Loss: 0.00007261, Val Loss: 0.00095320\n",
      "Epoch 238/500 - Train Loss: 0.00006883, Val Loss: 0.00170032\n",
      "Epoch 239/500 - Train Loss: 0.00006362, Val Loss: 0.00103467\n",
      "Epoch 240/500 - Train Loss: 0.00007639, Val Loss: 0.00079437\n",
      "Epoch 241/500 - Train Loss: 0.00006479, Val Loss: 0.00094948\n",
      "Epoch 242/500 - Train Loss: 0.00006890, Val Loss: 0.00140675\n",
      "Epoch 243/500 - Train Loss: 0.00007465, Val Loss: 0.00104736\n",
      "Epoch 244/500 - Train Loss: 0.00006335, Val Loss: 0.00109459\n",
      "Epoch 245/500 - Train Loss: 0.00006204, Val Loss: 0.00114449\n",
      "Epoch 246/500 - Train Loss: 0.00005604, Val Loss: 0.00114449\n",
      "Epoch 247/500 - Train Loss: 0.00006605, Val Loss: 0.00115004\n",
      "Epoch 248/500 - Train Loss: 0.00007207, Val Loss: 0.00101451\n",
      "Epoch 249/500 - Train Loss: 0.00006986, Val Loss: 0.00096559\n",
      "Epoch 250/500 - Train Loss: 0.00006845, Val Loss: 0.00098508\n",
      "Epoch 251/500 - Train Loss: 0.00006741, Val Loss: 0.00065995\n",
      "Epoch 252/500 - Train Loss: 0.00006292, Val Loss: 0.00074457\n",
      "Epoch 253/500 - Train Loss: 0.00007020, Val Loss: 0.00124266\n",
      "Epoch 254/500 - Train Loss: 0.00006829, Val Loss: 0.00170135\n",
      "Epoch 255/500 - Train Loss: 0.00007421, Val Loss: 0.00076859\n",
      "Epoch 256/500 - Train Loss: 0.00007241, Val Loss: 0.00143145\n",
      "Epoch 257/500 - Train Loss: 0.00006866, Val Loss: 0.00057087\n",
      "Epoch 258/500 - Train Loss: 0.00007185, Val Loss: 0.00153459\n",
      "Epoch 259/500 - Train Loss: 0.00007771, Val Loss: 0.00071583\n",
      "Epoch 260/500 - Train Loss: 0.00008548, Val Loss: 0.00292542\n",
      "Epoch 261/500 - Train Loss: 0.00009272, Val Loss: 0.00100922\n",
      "Epoch 262/500 - Train Loss: 0.00009423, Val Loss: 0.00256046\n",
      "Epoch 263/500 - Train Loss: 0.00012734, Val Loss: 0.00063929\n",
      "Epoch 264/500 - Train Loss: 0.00022749, Val Loss: 0.00157340\n",
      "Epoch 265/500 - Train Loss: 0.00010674, Val Loss: 0.00213295\n",
      "Epoch 266/500 - Train Loss: 0.00009750, Val Loss: 0.00075473\n",
      "Epoch 267/500 - Train Loss: 0.00013287, Val Loss: 0.00126139\n",
      "Epoch 268/500 - Train Loss: 0.00011605, Val Loss: 0.00166434\n",
      "Epoch 269/500 - Train Loss: 0.00013575, Val Loss: 0.00814644\n",
      "Epoch 270/500 - Train Loss: 0.00012178, Val Loss: 0.00126998\n",
      "Epoch 271/500 - Train Loss: 0.00016798, Val Loss: 0.00131685\n",
      "Epoch 272/500 - Train Loss: 0.00019678, Val Loss: 0.00838290\n",
      "Epoch 273/500 - Train Loss: 0.00019821, Val Loss: 0.00858611\n",
      "Epoch 274/500 - Train Loss: 0.00020088, Val Loss: 0.00194634\n",
      "Epoch 275/500 - Train Loss: 0.00025936, Val Loss: 0.00972947\n",
      "Epoch 276/500 - Train Loss: 0.00021456, Val Loss: 0.00374836\n",
      "Epoch 277/500 - Train Loss: 0.00018406, Val Loss: 0.00416187\n",
      "Epoch 278/500 - Train Loss: 0.00015265, Val Loss: 0.00132414\n",
      "Epoch 279/500 - Train Loss: 0.00023220, Val Loss: 0.01029111\n",
      "Epoch 280/500 - Train Loss: 0.00027141, Val Loss: 0.00779296\n",
      "Epoch 281/500 - Train Loss: 0.00026422, Val Loss: 0.00538124\n",
      "Epoch 282/500 - Train Loss: 0.00040731, Val Loss: 0.03918960\n",
      "Epoch 283/500 - Train Loss: 0.00094944, Val Loss: 0.00934248\n",
      "Epoch 284/500 - Train Loss: 0.00019846, Val Loss: 0.00982600\n",
      "Epoch 285/500 - Train Loss: 0.00018724, Val Loss: 0.01266753\n",
      "Epoch 286/500 - Train Loss: 0.00018163, Val Loss: 0.00636998\n",
      "Epoch 287/500 - Train Loss: 0.00021958, Val Loss: 0.00720926\n",
      "Epoch 288/500 - Train Loss: 0.00019640, Val Loss: 0.05051719\n",
      "Epoch 289/500 - Train Loss: 0.00075315, Val Loss: 0.02391634\n",
      "Epoch 290/500 - Train Loss: 0.00021979, Val Loss: 0.00148525\n",
      "Epoch 291/500 - Train Loss: 0.00022044, Val Loss: 0.00558535\n",
      "Epoch 292/500 - Train Loss: 0.00020608, Val Loss: 0.00223504\n",
      "Epoch 293/500 - Train Loss: 0.00020391, Val Loss: 0.00245383\n",
      "Epoch 294/500 - Train Loss: 0.00014192, Val Loss: 0.00260807\n",
      "Epoch 295/500 - Train Loss: 0.00018338, Val Loss: 0.00201904\n",
      "Epoch 296/500 - Train Loss: 0.00012096, Val Loss: 0.01921670\n",
      "Epoch 297/500 - Train Loss: 0.00031846, Val Loss: 0.00463039\n",
      "Epoch 298/500 - Train Loss: 0.00037127, Val Loss: 0.00196256\n",
      "Epoch 299/500 - Train Loss: 0.00026746, Val Loss: 0.00299014\n",
      "Epoch 300/500 - Train Loss: 0.00028260, Val Loss: 0.00286009\n",
      "Epoch 301/500 - Train Loss: 0.00027036, Val Loss: 0.00471760\n",
      "Epoch 302/500 - Train Loss: 0.00014827, Val Loss: 0.00279535\n",
      "Epoch 303/500 - Train Loss: 0.00020650, Val Loss: 0.00213763\n",
      "Epoch 304/500 - Train Loss: 0.00014923, Val Loss: 0.00206818\n",
      "Epoch 305/500 - Train Loss: 0.00030105, Val Loss: 0.01777376\n",
      "Epoch 306/500 - Train Loss: 0.00020222, Val Loss: 0.00312943\n",
      "Epoch 307/500 - Train Loss: 0.00012026, Val Loss: 0.00174261\n",
      "Epoch 308/500 - Train Loss: 0.00012575, Val Loss: 0.00225690\n",
      "Epoch 309/500 - Train Loss: 0.00012550, Val Loss: 0.00149839\n",
      "Epoch 310/500 - Train Loss: 0.00016585, Val Loss: 0.00520125\n",
      "Epoch 311/500 - Train Loss: 0.00018381, Val Loss: 0.00525273\n",
      "Epoch 312/500 - Train Loss: 0.00027139, Val Loss: 0.00438332\n",
      "Epoch 313/500 - Train Loss: 0.00013102, Val Loss: 0.00515701\n",
      "Epoch 314/500 - Train Loss: 0.00009729, Val Loss: 0.00706725\n",
      "Epoch 315/500 - Train Loss: 0.00013486, Val Loss: 0.02449949\n",
      "Epoch 316/500 - Train Loss: 0.00012321, Val Loss: 0.00502244\n",
      "Epoch 317/500 - Train Loss: 0.00011121, Val Loss: 0.00303384\n",
      "Epoch 318/500 - Train Loss: 0.00012863, Val Loss: 0.00554504\n",
      "Epoch 319/500 - Train Loss: 0.00008720, Val Loss: 0.00150584\n",
      "Epoch 320/500 - Train Loss: 0.00012404, Val Loss: 0.00088552\n",
      "Epoch 321/500 - Train Loss: 0.00011155, Val Loss: 0.00196423\n",
      "Epoch 322/500 - Train Loss: 0.00008349, Val Loss: 0.00510104\n",
      "Epoch 323/500 - Train Loss: 0.00008265, Val Loss: 0.00167002\n",
      "Epoch 324/500 - Train Loss: 0.00007980, Val Loss: 0.00262128\n",
      "Epoch 325/500 - Train Loss: 0.00006452, Val Loss: 0.00100020\n",
      "Epoch 326/500 - Train Loss: 0.00008033, Val Loss: 0.00164782\n",
      "Epoch 327/500 - Train Loss: 0.00010370, Val Loss: 0.00315809\n",
      "Epoch 328/500 - Train Loss: 0.00006464, Val Loss: 0.00236523\n",
      "Epoch 329/500 - Train Loss: 0.00007321, Val Loss: 0.00275528\n",
      "Epoch 330/500 - Train Loss: 0.00006400, Val Loss: 0.00126392\n",
      "Epoch 331/500 - Train Loss: 0.00007362, Val Loss: 0.00239037\n",
      "Epoch 332/500 - Train Loss: 0.00007428, Val Loss: 0.00097360\n",
      "Epoch 333/500 - Train Loss: 0.00006280, Val Loss: 0.00074490\n",
      "Epoch 334/500 - Train Loss: 0.00006323, Val Loss: 0.00097824\n",
      "Epoch 335/500 - Train Loss: 0.00006935, Val Loss: 0.00260479\n",
      "Epoch 336/500 - Train Loss: 0.00006422, Val Loss: 0.00116407\n",
      "Epoch 337/500 - Train Loss: 0.00005518, Val Loss: 0.00119994\n",
      "Epoch 338/500 - Train Loss: 0.00006393, Val Loss: 0.00072379\n",
      "Epoch 339/500 - Train Loss: 0.00006667, Val Loss: 0.00078072\n",
      "Epoch 340/500 - Train Loss: 0.00005576, Val Loss: 0.00114646\n",
      "Epoch 341/500 - Train Loss: 0.00005477, Val Loss: 0.00122589\n",
      "Epoch 342/500 - Train Loss: 0.00005568, Val Loss: 0.00119885\n",
      "Epoch 343/500 - Train Loss: 0.00005845, Val Loss: 0.00121827\n",
      "Epoch 344/500 - Train Loss: 0.00005428, Val Loss: 0.00121827\n",
      "Epoch 345/500 - Train Loss: 0.00005902, Val Loss: 0.00113884\n",
      "Epoch 346/500 - Train Loss: 0.00006413, Val Loss: 0.00109662\n",
      "Epoch 347/500 - Train Loss: 0.00005656, Val Loss: 0.00099832\n",
      "Epoch 348/500 - Train Loss: 0.00006060, Val Loss: 0.00116189\n",
      "Epoch 349/500 - Train Loss: 0.00005804, Val Loss: 0.00164261\n",
      "Epoch 350/500 - Train Loss: 0.00006486, Val Loss: 0.00117012\n",
      "Epoch 351/500 - Train Loss: 0.00005982, Val Loss: 0.00119707\n",
      "Epoch 352/500 - Train Loss: 0.00006515, Val Loss: 0.00121354\n",
      "Epoch 353/500 - Train Loss: 0.00006773, Val Loss: 0.00062451\n",
      "Epoch 354/500 - Train Loss: 0.00005961, Val Loss: 0.00178609\n",
      "Epoch 355/500 - Train Loss: 0.00007092, Val Loss: 0.00060459\n",
      "Epoch 356/500 - Train Loss: 0.00006342, Val Loss: 0.00093980\n",
      "Epoch 357/500 - Train Loss: 0.00007410, Val Loss: 0.00113597\n",
      "Epoch 358/500 - Train Loss: 0.00006649, Val Loss: 0.00105307\n",
      "Epoch 359/500 - Train Loss: 0.00007839, Val Loss: 0.00081433\n",
      "Epoch 360/500 - Train Loss: 0.00009669, Val Loss: 0.00145899\n",
      "Epoch 361/500 - Train Loss: 0.00007489, Val Loss: 0.00131367\n",
      "Epoch 362/500 - Train Loss: 0.00006793, Val Loss: 0.00139262\n",
      "Epoch 363/500 - Train Loss: 0.00015285, Val Loss: 0.00092042\n",
      "Epoch 364/500 - Train Loss: 0.00008928, Val Loss: 0.00110812\n",
      "Epoch 365/500 - Train Loss: 0.00009001, Val Loss: 0.00209859\n",
      "Epoch 366/500 - Train Loss: 0.00011727, Val Loss: 0.00120452\n",
      "Epoch 367/500 - Train Loss: 0.00008556, Val Loss: 0.00374815\n",
      "Epoch 368/500 - Train Loss: 0.00009535, Val Loss: 0.00201418\n",
      "Epoch 369/500 - Train Loss: 0.00014321, Val Loss: 0.00221886\n",
      "Epoch 370/500 - Train Loss: 0.00015514, Val Loss: 0.02617671\n",
      "Epoch 371/500 - Train Loss: 0.00055636, Val Loss: 0.00175122\n",
      "Epoch 372/500 - Train Loss: 0.00032204, Val Loss: 0.00665948\n",
      "Epoch 373/500 - Train Loss: 0.00033358, Val Loss: 0.00488171\n",
      "Epoch 374/500 - Train Loss: 0.00016147, Val Loss: 0.00155930\n",
      "Epoch 375/500 - Train Loss: 0.00020006, Val Loss: 0.00350145\n",
      "Epoch 376/500 - Train Loss: 0.00019079, Val Loss: 0.00657923\n",
      "Epoch 377/500 - Train Loss: 0.00023714, Val Loss: 0.00517821\n",
      "Epoch 378/500 - Train Loss: 0.00029635, Val Loss: 0.00588306\n",
      "Epoch 379/500 - Train Loss: 0.00014584, Val Loss: 0.00858295\n",
      "Epoch 380/500 - Train Loss: 0.00037339, Val Loss: 0.00591602\n",
      "Epoch 381/500 - Train Loss: 0.00027969, Val Loss: 0.00870043\n",
      "Epoch 382/500 - Train Loss: 0.00024778, Val Loss: 0.02643170\n",
      "Epoch 383/500 - Train Loss: 0.00025448, Val Loss: 0.01212575\n",
      "Epoch 384/500 - Train Loss: 0.00018830, Val Loss: 0.00920217\n",
      "Epoch 385/500 - Train Loss: 0.00040565, Val Loss: 0.01496750\n",
      "Epoch 386/500 - Train Loss: 0.00017028, Val Loss: 0.01080051\n",
      "Epoch 387/500 - Train Loss: 0.00014404, Val Loss: 0.01332864\n",
      "Epoch 388/500 - Train Loss: 0.00019591, Val Loss: 0.00819537\n",
      "Epoch 389/500 - Train Loss: 0.00015054, Val Loss: 0.00303717\n",
      "Epoch 390/500 - Train Loss: 0.00016535, Val Loss: 0.00357683\n",
      "Epoch 391/500 - Train Loss: 0.00018846, Val Loss: 0.00683178\n",
      "Epoch 392/500 - Train Loss: 0.00019356, Val Loss: 0.01447915\n",
      "Epoch 393/500 - Train Loss: 0.00015903, Val Loss: 0.00946296\n",
      "Epoch 394/500 - Train Loss: 0.00014160, Val Loss: 0.00767131\n",
      "Epoch 395/500 - Train Loss: 0.00019301, Val Loss: 0.01493333\n",
      "Epoch 396/500 - Train Loss: 0.00020576, Val Loss: 0.01356381\n",
      "Epoch 397/500 - Train Loss: 0.00022946, Val Loss: 0.02896257\n",
      "Epoch 398/500 - Train Loss: 0.00034072, Val Loss: 0.00340451\n",
      "Epoch 399/500 - Train Loss: 0.00020300, Val Loss: 0.00958452\n",
      "Epoch 400/500 - Train Loss: 0.00016231, Val Loss: 0.01644403\n",
      "Epoch 401/500 - Train Loss: 0.00023886, Val Loss: 0.00324090\n",
      "Epoch 402/500 - Train Loss: 0.00016780, Val Loss: 0.01560181\n",
      "Epoch 403/500 - Train Loss: 0.00013301, Val Loss: 0.00600949\n",
      "Epoch 404/500 - Train Loss: 0.00018856, Val Loss: 0.00398513\n",
      "Epoch 405/500 - Train Loss: 0.00024973, Val Loss: 0.00202688\n",
      "Epoch 406/500 - Train Loss: 0.00012087, Val Loss: 0.00532994\n",
      "Epoch 407/500 - Train Loss: 0.00010167, Val Loss: 0.00256243\n",
      "Epoch 408/500 - Train Loss: 0.00012855, Val Loss: 0.00165174\n",
      "Epoch 409/500 - Train Loss: 0.00018144, Val Loss: 0.00339248\n",
      "Epoch 410/500 - Train Loss: 0.00039800, Val Loss: 0.00350743\n",
      "Epoch 411/500 - Train Loss: 0.00014447, Val Loss: 0.00564270\n",
      "Epoch 412/500 - Train Loss: 0.00010873, Val Loss: 0.00702606\n",
      "Epoch 413/500 - Train Loss: 0.00009722, Val Loss: 0.00314383\n",
      "Epoch 414/500 - Train Loss: 0.00012635, Val Loss: 0.00531161\n",
      "Epoch 415/500 - Train Loss: 0.00010899, Val Loss: 0.00936091\n",
      "Epoch 416/500 - Train Loss: 0.00013328, Val Loss: 0.00199021\n",
      "Epoch 417/500 - Train Loss: 0.00009729, Val Loss: 0.00463610\n",
      "Epoch 418/500 - Train Loss: 0.00009044, Val Loss: 0.00181317\n",
      "Epoch 419/500 - Train Loss: 0.00008298, Val Loss: 0.00354279\n",
      "Epoch 420/500 - Train Loss: 0.00008138, Val Loss: 0.00135115\n",
      "Epoch 421/500 - Train Loss: 0.00007761, Val Loss: 0.00369325\n",
      "Epoch 422/500 - Train Loss: 0.00007115, Val Loss: 0.00621074\n",
      "Epoch 423/500 - Train Loss: 0.00010654, Val Loss: 0.00198910\n",
      "Epoch 424/500 - Train Loss: 0.00007930, Val Loss: 0.00218714\n",
      "Epoch 425/500 - Train Loss: 0.00007568, Val Loss: 0.00130075\n",
      "Epoch 426/500 - Train Loss: 0.00006688, Val Loss: 0.00277876\n",
      "Epoch 427/500 - Train Loss: 0.00006236, Val Loss: 0.00085435\n",
      "Epoch 428/500 - Train Loss: 0.00007461, Val Loss: 0.00372040\n",
      "Epoch 429/500 - Train Loss: 0.00007705, Val Loss: 0.00235516\n",
      "Epoch 430/500 - Train Loss: 0.00005967, Val Loss: 0.00348311\n",
      "Epoch 431/500 - Train Loss: 0.00005942, Val Loss: 0.00314013\n",
      "Epoch 432/500 - Train Loss: 0.00005393, Val Loss: 0.00173423\n",
      "Epoch 433/500 - Train Loss: 0.00005600, Val Loss: 0.00148568\n",
      "Epoch 434/500 - Train Loss: 0.00006039, Val Loss: 0.00172692\n",
      "Epoch 435/500 - Train Loss: 0.00005637, Val Loss: 0.00222136\n",
      "Epoch 436/500 - Train Loss: 0.00006032, Val Loss: 0.00181722\n",
      "Epoch 437/500 - Train Loss: 0.00005819, Val Loss: 0.00168445\n",
      "Epoch 438/500 - Train Loss: 0.00005536, Val Loss: 0.00197493\n",
      "Epoch 439/500 - Train Loss: 0.00006325, Val Loss: 0.00148430\n",
      "Epoch 440/500 - Train Loss: 0.00005034, Val Loss: 0.00162145\n",
      "Epoch 441/500 - Train Loss: 0.00005659, Val Loss: 0.00165098\n",
      "Epoch 442/500 - Train Loss: 0.00005322, Val Loss: 0.00165098\n",
      "Epoch 443/500 - Train Loss: 0.00006109, Val Loss: 0.00161820\n",
      "Epoch 444/500 - Train Loss: 0.00005459, Val Loss: 0.00146467\n",
      "Epoch 445/500 - Train Loss: 0.00005542, Val Loss: 0.00147873\n",
      "Epoch 446/500 - Train Loss: 0.00005604, Val Loss: 0.00154095\n",
      "Epoch 447/500 - Train Loss: 0.00005700, Val Loss: 0.00205256\n",
      "Epoch 448/500 - Train Loss: 0.00005274, Val Loss: 0.00199379\n",
      "Epoch 449/500 - Train Loss: 0.00005171, Val Loss: 0.00188291\n",
      "Epoch 450/500 - Train Loss: 0.00005553, Val Loss: 0.00218325\n",
      "Epoch 451/500 - Train Loss: 0.00006849, Val Loss: 0.00135518\n",
      "Epoch 452/500 - Train Loss: 0.00005899, Val Loss: 0.00154501\n",
      "Epoch 453/500 - Train Loss: 0.00006759, Val Loss: 0.00200714\n",
      "Epoch 454/500 - Train Loss: 0.00005745, Val Loss: 0.00136865\n",
      "Epoch 455/500 - Train Loss: 0.00006522, Val Loss: 0.00312964\n",
      "Epoch 456/500 - Train Loss: 0.00006999, Val Loss: 0.00265676\n",
      "Epoch 457/500 - Train Loss: 0.00007722, Val Loss: 0.00332269\n",
      "Epoch 458/500 - Train Loss: 0.00007290, Val Loss: 0.00461566\n",
      "Epoch 459/500 - Train Loss: 0.00007302, Val Loss: 0.00267838\n",
      "Epoch 460/500 - Train Loss: 0.00008568, Val Loss: 0.00269632\n",
      "Epoch 461/500 - Train Loss: 0.00008845, Val Loss: 0.00111122\n",
      "Epoch 462/500 - Train Loss: 0.00009540, Val Loss: 0.00361831\n",
      "Epoch 463/500 - Train Loss: 0.00009425, Val Loss: 0.00734375\n",
      "Epoch 464/500 - Train Loss: 0.00009249, Val Loss: 0.00071582\n",
      "Epoch 465/500 - Train Loss: 0.00011333, Val Loss: 0.00992390\n",
      "Epoch 466/500 - Train Loss: 0.00008831, Val Loss: 0.00394952\n",
      "Epoch 467/500 - Train Loss: 0.00009840, Val Loss: 0.01039749\n",
      "Epoch 468/500 - Train Loss: 0.00012403, Val Loss: 0.00346642\n",
      "Epoch 469/500 - Train Loss: 0.00012449, Val Loss: 0.00140813\n",
      "Epoch 470/500 - Train Loss: 0.00013264, Val Loss: 0.00151339\n",
      "Epoch 471/500 - Train Loss: 0.00014542, Val Loss: 0.01309960\n",
      "Epoch 472/500 - Train Loss: 0.00017525, Val Loss: 0.01066921\n",
      "Epoch 473/500 - Train Loss: 0.00015457, Val Loss: 0.00256644\n",
      "Epoch 474/500 - Train Loss: 0.00011283, Val Loss: 0.00996870\n",
      "Epoch 475/500 - Train Loss: 0.00019886, Val Loss: 0.02850673\n",
      "Epoch 476/500 - Train Loss: 0.00035375, Val Loss: 0.01029637\n",
      "Epoch 477/500 - Train Loss: 0.00014604, Val Loss: 0.00974678\n",
      "Epoch 478/500 - Train Loss: 0.00011173, Val Loss: 0.00421224\n",
      "Epoch 479/500 - Train Loss: 0.00024974, Val Loss: 0.02022053\n",
      "Epoch 480/500 - Train Loss: 0.00017581, Val Loss: 0.01877001\n",
      "Epoch 481/500 - Train Loss: 0.00027404, Val Loss: 0.00171170\n",
      "Epoch 482/500 - Train Loss: 0.00019212, Val Loss: 0.00939063\n",
      "Epoch 483/500 - Train Loss: 0.00023589, Val Loss: 0.01880636\n",
      "Epoch 484/500 - Train Loss: 0.00032241, Val Loss: 0.01008586\n",
      "Epoch 485/500 - Train Loss: 0.00018267, Val Loss: 0.01422996\n",
      "Epoch 486/500 - Train Loss: 0.00022735, Val Loss: 0.01621681\n",
      "Epoch 487/500 - Train Loss: 0.00029712, Val Loss: 0.00286519\n",
      "Epoch 488/500 - Train Loss: 0.00031077, Val Loss: 0.02455638\n",
      "Epoch 489/500 - Train Loss: 0.00020501, Val Loss: 0.00235106\n",
      "Epoch 490/500 - Train Loss: 0.00023911, Val Loss: 0.00453246\n",
      "Epoch 491/500 - Train Loss: 0.00018410, Val Loss: 0.02172566\n",
      "Epoch 492/500 - Train Loss: 0.00018275, Val Loss: 0.00812784\n",
      "Epoch 493/500 - Train Loss: 0.00016391, Val Loss: 0.00625355\n",
      "Epoch 494/500 - Train Loss: 0.00014806, Val Loss: 0.00205850\n",
      "Epoch 495/500 - Train Loss: 0.00017639, Val Loss: 0.01086844\n",
      "Epoch 496/500 - Train Loss: 0.00023414, Val Loss: 0.00956892\n",
      "Epoch 497/500 - Train Loss: 0.00026158, Val Loss: 0.00427527\n",
      "Epoch 498/500 - Train Loss: 0.00016186, Val Loss: 0.02357113\n",
      "Epoch 499/500 - Train Loss: 0.00024998, Val Loss: 0.00295350\n",
      "Epoch 500/500 - Train Loss: 0.00020255, Val Loss: 0.00242751\n",
      "Loading overall best model from epoch 138\n",
      "\n",
      "Model Evaluation Metrics:\n",
      "MSE                 : 0.001093\n",
      "RMSE                : 0.033065\n",
      "MAE                 : 0.025267\n",
      "R2                  : 0.997724\n",
      "MAPE                : 0.475009\n",
      "EXPLAINED_VARIANCE  : 0.997745\n",
      "MAX_ERROR           : 0.123473\n",
      "\n",
      "Final lstm Model Performance:\n",
      "Validation RMSE: 0.03306548\n",
      "Validation MAE: 0.02526699\n",
      "Validation RÂ²: 0.99772400\n",
      "Validation MAPE: 0.4750%\n",
      "\n",
      "Final model and results saved to Results\n",
      "Best model achieved at epoch 138\n",
      "Interactive visualizations saved as HTML files for better exploration\n",
      "Total training time: 293.87 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from Models.create_model_with_params import create_model_with_params\n",
    "from Models.train_final_model import train_final_model\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "Trained_Model = train_final_model(\n",
    "    best_params=best_params,\n",
    "    model_type=\"lstm\",\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,  # explicitly no validation\n",
    "    num_features=num_features,\n",
    "    scaler_y=y_scaler,\n",
    "    input_features=feature_names,\n",
    "    data=data,\n",
    "    output_dir=\"Results\",\n",
    ")\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print(f\"Total training time: {elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb580ed-397c-4278-9b9d-8e1aeb2d56bb",
   "metadata": {},
   "source": [
    "#### 10. Test Set Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4443c87e-9b1e-4178-ab6e-97262f5934d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Evaluation Metrics:\n",
      "MSE                 : 0.017329\n",
      "RMSE                : 0.131639\n",
      "MAE                 : 0.094420\n",
      "R2                  : 0.986200\n",
      "MAPE                : 1.670436\n",
      "EXPLAINED_VARIANCE  : 0.986389\n",
      "MAX_ERROR           : 0.626125\n"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# Imports\n",
    "# ====================================\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Custom modules\n",
    "from Models.evaluate_model import evaluate_model\n",
    "from Optimization.select_features_for_model import select_features_for_model\n",
    "from PlotScripts.get_time_series_comparison_plot import time_series_comparison_plot\n",
    "from PlotScripts.get_scatter_plot import scatter_plot\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Step 1: Data Preparation\n",
    "# ====================================\n",
    "df = test_data_ready.copy()  # Work on a copy to preserve original\n",
    "valid_features = select_features_for_model(df, \"lstm\")  # Select features suitable for LSTM\n",
    "seq_length = 30  # Length of input sequences\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Step 2: Feature & Target Scaling\n",
    "# ====================================\n",
    "X_all = X_scaler.transform(df[valid_features].values)   # Feature scaling\n",
    "y_all = df[\"Close\"].values.reshape(-1, 1)               # Target variable\n",
    "y_all_scaled = y_scaler.transform(y_all)                # Target scaling\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Step 3: Create Sliding Windows (Sequence Generation)\n",
    "# ====================================\n",
    "X_windows = []  # Sequences of input features\n",
    "y_targets = []  # Corresponding next-day targets\n",
    "\n",
    "for end_ix in range(seq_length, len(X_all)):\n",
    "    start_ix = end_ix - seq_length\n",
    "    X_windows.append(X_all[start_ix:end_ix])\n",
    "    y_targets.append(y_all_scaled[end_ix])\n",
    "\n",
    "X_tensor = torch.tensor(X_windows, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_targets, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Step 4: DataLoader Creation\n",
    "# ====================================\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "loader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Step 5: Model Evaluation\n",
    "# ====================================\n",
    "metrics = evaluate_model(Trained_Model, data_loader=loader)\n",
    "\n",
    "# ====================================\n",
    "# Step 6: Model Inference for Plotting\n",
    "# ====================================\n",
    "Trained_Model.eval()  # Set model to evaluation mode\n",
    "X_tensor = X_tensor.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred_scaled = Trained_Model(X_tensor).view(-1, 1).cpu().numpy()\n",
    "\n",
    "# Inverse scale to get actual price values\n",
    "y_pred = y_scaler.inverse_transform(y_pred_scaled).flatten()\n",
    "y_true = y_all[seq_length:].flatten()\n",
    "dates = df.index[seq_length:]\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Step 7: Visualization\n",
    "# ====================================\n",
    "time_series_comparison_plot(\n",
    "    targets_original=y_true,\n",
    "    predictions_original=y_pred,\n",
    "    model_type=\"LSTM\",\n",
    "    output_dir=\"Results\",\n",
    "    phase=\"Testing\"\n",
    ")\n",
    "scatter_plot(\n",
    "    targets_original=y_true,\n",
    "    predictions_original=y_pred,\n",
    "    model_type=\"LSTM\",\n",
    "    output_dir=\"Results\",\n",
    "    phase=\"Testing\"\n",
    ")\n",
    "\n",
    "# ====================================\n",
    "# Step 8: Store and Print Results\n",
    "# ====================================\n",
    "results = {\n",
    "    f\"{ticker}_predictions\": y_pred,\n",
    "    f\"{ticker}_actuals\": y_true,\n",
    "    **{f\"{ticker}_{k}\": v for k, v in metrics.items()},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8aca96-7db9-4e6d-b878-841a9a821565",
   "metadata": {},
   "source": [
    "####  11. Generate Forecast "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6adec42e-b367-4a3d-a6a6-014943f76260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Close Price of INFY.NS for 2025-06-02: 1566.3815\n"
     ]
    }
   ],
   "source": [
    "# Import the forecasting function from the utility module\n",
    "from Utils.get_forecast import generate_forecast\n",
    "\n",
    "# Create a copy of the test data to avoid modifying the original dataset\n",
    "forecast_data = test_data_ready.copy()\n",
    "\n",
    "# Generate forecast using the trained LSTM model\n",
    "generate_forecast(\n",
    "    forecast_data=forecast_data,  # Input data prepared for forecasting\n",
    "    Trained_Model=Trained_Model,  # The trained model used for prediction\n",
    "    X_scaler=X_scaler,  # Scaler used to normalize input features\n",
    "    y_scaler=y_scaler,  # Scaler used to denormalize output predictions\n",
    "    seq_length=30,  # Number of time steps to look back for each prediction\n",
    "    ticker=ticker,  # Ticker symbol for the stock (or identifier for the time series)\n",
    "    model=\"lstm\",  # Type of model to be used (\"lstm\" in this case)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38c8b25-b497-4013-9229-6617cac01e36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
