{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a0c8d45-c5a3-493d-969a-120839816798",
   "metadata": {},
   "source": [
    "####  1. Select Stock Ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96720a6b-e4c8-4480-85ad-61438be2cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set the ticker symbol\n",
    "ticker = \"INFY.NS\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4a2103-01ea-49a4-a4e5-5303e5530c15",
   "metadata": {},
   "source": [
    "#### 2. Select appropriate device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "816bb42a-e447-4405-963f-e4ee5f83e50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Select appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a47ee33-eef3-45f2-bff1-ac4a61514a41",
   "metadata": {},
   "source": [
    "#### 3. Run the CUDA check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12c5d306-f730-439b-9aeb-41e36512fbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available with 1 device(s).\n",
      "Current device: 0 - NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "CUDA version: 12.1\n",
      "Total GPU memory     : 4.29444301 GB\n",
      "Currently allocated  : 0.00000000 GB\n",
      "Currently reserved   : 0.00000000 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Config.check_cuda_config import check_cuda_configuration\n",
    "\n",
    "# Run the CUDA check\n",
    "check_cuda_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cb156e-1d9f-4a1b-ae36-95109d94b189",
   "metadata": {},
   "source": [
    "#### 4. Fetch stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2c3bbb2-3af3-477a-8929-2e5cb3d379e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched raw data saved to: Data/RawData\\INFY.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>232.496428</td>\n",
       "      <td>234.278010</td>\n",
       "      <td>231.186970</td>\n",
       "      <td>232.728043</td>\n",
       "      <td>4069264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>233.387211</td>\n",
       "      <td>234.340353</td>\n",
       "      <td>231.614546</td>\n",
       "      <td>233.507477</td>\n",
       "      <td>6895528</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>238.999244</td>\n",
       "      <td>238.999244</td>\n",
       "      <td>229.387617</td>\n",
       "      <td>230.100250</td>\n",
       "      <td>6817288</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>230.411991</td>\n",
       "      <td>230.411991</td>\n",
       "      <td>224.318968</td>\n",
       "      <td>224.929169</td>\n",
       "      <td>10892600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>224.390205</td>\n",
       "      <td>224.773249</td>\n",
       "      <td>218.956381</td>\n",
       "      <td>219.508667</td>\n",
       "      <td>12649312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close    Volume  \\\n",
       "2010-01-04  232.496428  234.278010  231.186970  232.728043   4069264   \n",
       "2010-01-05  233.387211  234.340353  231.614546  233.507477   6895528   \n",
       "2010-01-06  238.999244  238.999244  229.387617  230.100250   6817288   \n",
       "2010-01-07  230.411991  230.411991  224.318968  224.929169  10892600   \n",
       "2010-01-08  224.390205  224.773249  218.956381  219.508667  12649312   \n",
       "\n",
       "            Dividends  Stock Splits  \n",
       "2010-01-04        0.0           0.0  \n",
       "2010-01-05        0.0           0.0  \n",
       "2010-01-06        0.0           0.0  \n",
       "2010-01-07        0.0           0.0  \n",
       "2010-01-08        0.0           0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from DataPipeline.data_fetcher import fetch_daily_data_ist\n",
    "\n",
    "# Create the path if it doesn't exist\n",
    "raw_data_dir = \"Data/RawData\"\n",
    "\n",
    "# Fetch stock data\n",
    "data = fetch_daily_data_ist(ticker)\n",
    "\n",
    "# Construct filename and full path\n",
    "filename = f\"{ticker[:-3]}.csv\"\n",
    "filepath = os.path.join(raw_data_dir, filename)\n",
    "\n",
    "# Save to CSV\n",
    "data.to_csv(filepath)\n",
    "print(f\"Fetched raw data saved to: {filepath}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb345f3-45aa-4f53-9a3d-aba0096e8fce",
   "metadata": {},
   "source": [
    "#### 5. Calculate Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63c463d7-703b-4952-a89e-72a7fcfb1062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataPipeline.technical_indicators import TechnicalIndicators\n",
    "\n",
    "# Calculate technical indicators\n",
    "indicators = TechnicalIndicators(data)\n",
    "indicators_data = indicators.calculate_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5232e794-c0ff-4b23-bd44-3b9f325ebe65",
   "metadata": {},
   "source": [
    "#### 6. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87703c18-5e35-4d98-9ffb-5ef3eb597394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed data shape after indicators: (3802, 81)\n",
      "\n",
      "Data split sizes:\n",
      "\n",
      "Train: 3231, Test: 571\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nProcessed data shape after indicators: {indicators_data.shape}\")\n",
    "\n",
    "# Split data into train and validation (85%) and test (15%)\n",
    "total_size = len(indicators_data)\n",
    "train_size = int(0.85 * total_size)\n",
    "\n",
    "train_data = indicators_data[:train_size].copy()\n",
    "test_data = indicators_data[train_size:].copy()\n",
    "\n",
    "print(f\"\\nData split sizes:\")\n",
    "print(f\"\\nTrain: {len(train_data)}, Test: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39821baf-ab1c-4f82-89a4-9f6a8a744bed",
   "metadata": {},
   "source": [
    "#### 7. Dropping NaN Columns and Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e72d01b-aabd-47d2-aebe-88119f5a4fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No columns with all null values found.\n",
      "No columns with all null values found.\n",
      "\n",
      "Dropped columns due to nulls: []\n",
      "Final shapes - Train: (3204, 81), Test: (570, 81)\n"
     ]
    }
   ],
   "source": [
    "from DataPipeline.data_cleaning import drop_all_null_columns\n",
    "\n",
    "# Drop all-null columns\n",
    "train_data_ready, dropped_columns = drop_all_null_columns(train_data)\n",
    "test_data_ready, _ = drop_all_null_columns(test_data)\n",
    "\n",
    "print(f\"\\nDropped columns due to nulls: {dropped_columns}\")\n",
    "print(f\"Final shapes - Train: {train_data_ready.shape}, Test: {test_data_ready.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b08eca9-e706-4fc3-943a-cb771290ce88",
   "metadata": {},
   "source": [
    "#### 8. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fc83a6a-426d-429b-8db6-7c5e34d0dd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Optimization.prepare_data_for_hpo import prepare_data_for_hpo\n",
    "\n",
    "# Prepare data for LSTM (only train loader, no val)\n",
    "data, train_loader, val_loader, X_scaler, y_scaler, feature_names, num_features = prepare_data_for_hpo(\n",
    "    Modelling_data=train_data_ready,\n",
    "    model_type=\"LSTM\",\n",
    "    batch_size=64,\n",
    "    seq_length=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e08e064-74c5-4171-9af8-4110adf93a7e",
   "metadata": {},
   "source": [
    "#### 9. Hyper Parameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "557d1260-a1fa-431e-8497-18a3bd2e5d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-31 03:44:43,411] A new study created in memory with name: lstm_multifidelity_hpo_20250531_034443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Bayesian HPO for LSTM model with 2 trials\n",
      "Data file:                    Open         High          Low        Close    Volume  \\\n",
      "2010-02-04   219.579937   220.025332   214.903285   216.377548   6176408   \n",
      "2010-02-05   212.934601   213.651692   207.465155   209.576324  13544872   \n",
      "2010-02-08   211.553872   217.531086   208.845877   214.110458   9252480   \n",
      "2010-02-09   214.146092   221.094259   213.277571   220.363815  10387392   \n",
      "2010-02-10   221.896065   222.359272   218.065663   219.753708   7187984   \n",
      "...                 ...          ...          ...          ...       ...   \n",
      "2023-01-25  1461.103096  1469.603373  1453.311176  1457.325195   4158617   \n",
      "2023-01-27  1460.158697  1464.833804  1423.701977  1434.799561   6209955   \n",
      "2023-01-30  1443.913768  1458.269746  1435.602340  1453.358521   6964719   \n",
      "2023-01-31  1462.897567  1462.897567  1428.990886  1448.588745   9448126   \n",
      "2023-02-01  1456.569612  1468.706210  1443.205380  1464.975464   6194199   \n",
      "\n",
      "            Dividends  Stock Splits  Daily_Range  Daily_Change  Close_Lag_1  \\\n",
      "2010-02-04        0.0           0.0     5.122047     -3.202389   220.310440   \n",
      "2010-02-05        0.0           0.0     6.186537     -3.358277   216.377548   \n",
      "2010-02-08        0.0           0.0     8.685209      2.556587   209.576324   \n",
      "2010-02-09        0.0           0.0     7.816689      6.217724   214.110458   \n",
      "2010-02-10        0.0           0.0     4.293609     -2.142357   220.363815   \n",
      "...               ...           ...          ...           ...          ...   \n",
      "2023-01-25        0.0           0.0    16.292197     -3.777901  1465.731079   \n",
      "2023-01-27        0.0           0.0    41.131827    -25.359137  1457.325195   \n",
      "2023-01-30        0.0           0.0    22.667405      9.444752  1434.799561   \n",
      "2023-01-31        0.0           0.0    33.906681    -14.308821  1453.358521   \n",
      "2023-02-01        0.0           0.0    25.500830      8.405852  1448.588745   \n",
      "\n",
      "            ...  Momentum_7_lag1  Momentum_14_lag1  Momentum_21_lag1  \\\n",
      "2010-02-04  ...        -9.121704        -18.733353        -12.417603   \n",
      "2010-02-05  ...       -10.088181        -23.222916        -17.129929   \n",
      "2010-02-08  ...       -13.321808        -28.781433        -20.523926   \n",
      "2010-02-09  ...        -7.852341        -25.213837        -10.818710   \n",
      "2010-02-10  ...        -0.151428        -14.439682          0.855148   \n",
      "...         ...              ...               ...               ...   \n",
      "2023-01-25  ...        45.570923         53.929565         46.751587   \n",
      "2023-01-27  ...        16.528198         64.035522         26.586914   \n",
      "2023-01-30  ...       -19.219971         66.727173          8.500488   \n",
      "2023-01-31  ...        -6.233521         51.332397         20.070068   \n",
      "2023-02-01  ...        -4.864136         60.446411         24.131348   \n",
      "\n",
      "            RSI_2_lag1  RSI_3_lag1  RSI_5_lag1  RSI_7_lag1  RSI_14_lag1  \\\n",
      "2010-02-04   66.552475   47.197987   35.283069   34.238589    36.655021   \n",
      "2010-02-05   24.075898   24.786668   24.779866   26.921052    32.250609   \n",
      "2010-02-08    7.506249   11.106556   15.077606   18.810228    26.353352   \n",
      "2010-02-09   51.766659   42.720058   35.969512   34.223584    34.899813   \n",
      "2010-02-10   79.209167   66.999334   55.038434   49.614572    44.470830   \n",
      "...                ...         ...         ...         ...          ...   \n",
      "2023-01-25   79.816007   75.283533   71.131085   66.946989    57.605257   \n",
      "2023-01-27   42.899773   54.294187   60.533866   60.261090    54.992147   \n",
      "2023-01-30   12.331580   25.602185   40.381730   45.922794    48.626444   \n",
      "2023-01-31   59.676811   54.994794   55.603314   55.988611    53.411487   \n",
      "2023-02-01   46.710373   47.725910   51.388300   53.028970    52.069168   \n",
      "\n",
      "            RSI_21_lag1       Date  \n",
      "2010-02-04    37.967817 2010-02-04  \n",
      "2010-02-05    34.357217 2010-02-05  \n",
      "2010-02-08    29.298172 2010-02-08  \n",
      "2010-02-09    35.904701 2010-02-09  \n",
      "2010-02-10    43.544116 2010-02-10  \n",
      "...                 ...        ...  \n",
      "2023-01-25    54.354155 2023-01-25  \n",
      "2023-01-27    52.787077 2023-01-27  \n",
      "2023-01-30    48.826180 2023-01-30  \n",
      "2023-01-31    51.945540 2023-01-31  \n",
      "2023-02-01    51.104893 2023-02-01  \n",
      "\n",
      "[3204 rows x 81 columns]\n",
      "Output directory: Results/HPO\n",
      "Starting Bayesian Multi-Fidelity HPO with 2 trials for lstm model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "948219312e9d4bab9685fa9c088b0039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/20 - Train Loss: 0.73248730, Val Loss: 5.30984044\n",
      "Epoch 10/20 - Train Loss: 0.67191773, Val Loss: 5.36524117\n",
      "Early stopping at epoch 10\n",
      "\n",
      "Model Evaluation Metrics:\n",
      "MSE                 : 29.770412\n",
      "RMSE                : 5.456227\n",
      "MAE                 : 5.420729\n",
      "R2                  : -75.384056\n",
      "MAPE                : 100.600278\n",
      "EXPLAINED_VARIANCE  : 0.009341\n",
      "MAX_ERROR           : 6.894820\n",
      "[I 2025-05-31 03:44:47,899] Trial 0 finished with value: 5.456226942225586 and parameters: {'hidden_size': 403, 'num_layers': 3, 'dropout': 0.36599697090570255, 'learning_rate': 0.0006251373574521745, 'weight_decay': 2.9380279387035354e-06, 'batch_norm': True, 'cell_dropout': 0.4330880728874676, 'optimizer': 'sgd', 'lr_scheduler': 'none', 'loss_function': 'mae'}. Best is trial 0 with value: 5.456226942225586.\n",
      "Epoch 0/178 - Train Loss: 0.73487945, Val Loss: 5.35169423\n",
      "Epoch 10/178 - Train Loss: 0.72684696, Val Loss: 5.38345969\n",
      "Epoch 20/178 - Train Loss: 0.72453008, Val Loss: 5.39666748\n",
      "Epoch 30/178 - Train Loss: 0.72131442, Val Loss: 5.40245438\n",
      "Early stopping at epoch 35\n",
      "\n",
      "Model Evaluation Metrics:\n",
      "MSE                 : 30.208328\n",
      "RMSE                : 5.496210\n",
      "MAE                 : 5.460850\n",
      "R2                  : -76.507645\n",
      "MAPE                : 101.352620\n",
      "EXPLAINED_VARIANCE  : 0.005913\n",
      "MAX_ERROR           : 6.942908\n",
      "[I 2025-05-31 03:45:05,663] Trial 1 finished with value: 5.496210353240705 and parameters: {'hidden_size': 321, 'num_layers': 2, 'dropout': 0.06974693032602092, 'learning_rate': 7.52374288453485e-05, 'weight_decay': 1.2562773503807034e-05, 'batch_norm': False, 'cell_dropout': 0.09983689107917987, 'optimizer': 'sgd', 'lr_scheduler': 'plateau', 'lr_factor': 0.746717878493169, 'lr_patience': 6, 'loss_function': 'mae'}. Best is trial 0 with value: 5.456226942225586.\n",
      "\n",
      "Hyperparameter Optimization Complete!\n",
      "Best lstm params: {'hidden_size': 403, 'num_layers': 3, 'dropout': 0.36599697090570255, 'learning_rate': 0.0006251373574521745, 'weight_decay': 2.9380279387035354e-06, 'batch_norm': True, 'cell_dropout': 0.4330880728874676, 'optimizer': 'sgd', 'lr_scheduler': 'none', 'loss_function': 'mae'}\n",
      "Best validation RMSE: 5.45622694\n",
      "Could not create optimization history plot: 'Figure' object has no attribute 'savefig'\n",
      "Could not create parameter importance plot: 'Figure' object has no attribute 'savefig'\n",
      "Could not create parallel coordinate plot: 'Figure' object has no attribute 'savefig'\n",
      "Optimization results saved to Results/HPO\n",
      "\n",
      "Training final model with best hyperparameters...\n",
      " {'hidden_size': 403, 'num_layers': 3, 'dropout': 0.36599697090570255, 'learning_rate': 0.0006251373574521745, 'weight_decay': 2.9380279387035354e-06, 'batch_norm': True, 'cell_dropout': 0.4330880728874676, 'optimizer': 'sgd', 'lr_scheduler': 'none', 'loss_function': 'mae'}\n",
      "Epoch 1/500 - Train Loss: 0.73135618, Val Loss: 5.40801418 (Best Model Saved)\n",
      "Epoch 2/500 - Train Loss: 0.72501177, Val Loss: 5.41490257\n",
      "Epoch 3/500 - Train Loss: 0.71811088, Val Loss: 5.41436446\n",
      "Epoch 4/500 - Train Loss: 0.71305387, Val Loss: 5.40689707 (Best Model Saved)\n",
      "Epoch 5/500 - Train Loss: 0.70819352, Val Loss: 5.39601851 (Best Model Saved)\n",
      "Epoch 6/500 - Train Loss: 0.70217206, Val Loss: 5.38403666 (Best Model Saved)\n",
      "Epoch 7/500 - Train Loss: 0.69778026, Val Loss: 5.36397004 (Best Model Saved)\n",
      "Epoch 8/500 - Train Loss: 0.69022115, Val Loss: 5.34304452 (Best Model Saved)\n",
      "Epoch 9/500 - Train Loss: 0.68538660, Val Loss: 5.31656110 (Best Model Saved)\n",
      "Epoch 10/500 - Train Loss: 0.67806028, Val Loss: 5.28685892 (Best Model Saved)\n",
      "Epoch 11/500 - Train Loss: 0.67068330, Val Loss: 5.25365591 (Best Model Saved)\n",
      "Epoch 12/500 - Train Loss: 0.66325451, Val Loss: 5.22069597 (Best Model Saved)\n",
      "Epoch 13/500 - Train Loss: 0.65266828, Val Loss: 5.17286444 (Best Model Saved)\n",
      "Epoch 14/500 - Train Loss: 0.64007331, Val Loss: 5.11859381 (Best Model Saved)\n",
      "Epoch 15/500 - Train Loss: 0.62577314, Val Loss: 5.05509758 (Best Model Saved)\n",
      "Epoch 16/500 - Train Loss: 0.60730907, Val Loss: 4.97996891 (Best Model Saved)\n",
      "Epoch 17/500 - Train Loss: 0.58582420, Val Loss: 4.88351011 (Best Model Saved)\n",
      "Epoch 18/500 - Train Loss: 0.55800624, Val Loss: 4.77171671 (Best Model Saved)\n",
      "Epoch 19/500 - Train Loss: 0.52409637, Val Loss: 4.63901889 (Best Model Saved)\n",
      "Epoch 20/500 - Train Loss: 0.48133589, Val Loss: 4.46921277 (Best Model Saved)\n",
      "Epoch 21/500 - Train Loss: 0.42198509, Val Loss: 4.25772822 (Best Model Saved)\n",
      "Epoch 22/500 - Train Loss: 0.35288358, Val Loss: 4.00795096 (Best Model Saved)\n",
      "Epoch 23/500 - Train Loss: 0.27598281, Val Loss: 3.71594405 (Best Model Saved)\n",
      "Epoch 24/500 - Train Loss: 0.21484737, Val Loss: 3.47443420 (Best Model Saved)\n",
      "Epoch 25/500 - Train Loss: 0.19397994, Val Loss: 3.36267918 (Best Model Saved)\n",
      "Epoch 26/500 - Train Loss: 0.18975824, Val Loss: 3.32490134 (Best Model Saved)\n",
      "Epoch 27/500 - Train Loss: 0.17501776, Val Loss: 3.29087687 (Best Model Saved)\n",
      "Epoch 28/500 - Train Loss: 0.18117440, Val Loss: 3.26955491 (Best Model Saved)\n",
      "Epoch 29/500 - Train Loss: 0.17815025, Val Loss: 3.24796593 (Best Model Saved)\n",
      "Epoch 30/500 - Train Loss: 0.16395820, Val Loss: 3.23077679 (Best Model Saved)\n",
      "Epoch 31/500 - Train Loss: 0.16293033, Val Loss: 3.21457905 (Best Model Saved)\n",
      "Epoch 32/500 - Train Loss: 0.16483600, Val Loss: 3.18767744 (Best Model Saved)\n",
      "Epoch 33/500 - Train Loss: 0.17054008, Val Loss: 3.14962149 (Best Model Saved)\n",
      "Epoch 34/500 - Train Loss: 0.15786308, Val Loss: 3.12786651 (Best Model Saved)\n",
      "Epoch 35/500 - Train Loss: 0.15103241, Val Loss: 3.08761793 (Best Model Saved)\n",
      "Epoch 36/500 - Train Loss: 0.15530738, Val Loss: 3.05789644 (Best Model Saved)\n",
      "Epoch 37/500 - Train Loss: 0.15829049, Val Loss: 3.00704426 (Best Model Saved)\n",
      "Epoch 38/500 - Train Loss: 0.15505165, Val Loss: 2.98621345 (Best Model Saved)\n",
      "Epoch 39/500 - Train Loss: 0.14390480, Val Loss: 2.95232189 (Best Model Saved)\n",
      "Epoch 40/500 - Train Loss: 0.14844689, Val Loss: 2.92042500 (Best Model Saved)\n",
      "Epoch 41/500 - Train Loss: 0.14581889, Val Loss: 2.90348715 (Best Model Saved)\n",
      "Epoch 42/500 - Train Loss: 0.13438431, Val Loss: 2.84386623 (Best Model Saved)\n",
      "Epoch 43/500 - Train Loss: 0.15078688, Val Loss: 2.76773369 (Best Model Saved)\n",
      "Epoch 44/500 - Train Loss: 0.13087691, Val Loss: 2.76910675\n",
      "Epoch 45/500 - Train Loss: 0.12990444, Val Loss: 2.74472576 (Best Model Saved)\n",
      "Epoch 46/500 - Train Loss: 0.13590413, Val Loss: 2.70171410 (Best Model Saved)\n",
      "Epoch 47/500 - Train Loss: 0.14078040, Val Loss: 2.69574201 (Best Model Saved)\n",
      "Epoch 48/500 - Train Loss: 0.12701102, Val Loss: 2.61199838 (Best Model Saved)\n",
      "Epoch 49/500 - Train Loss: 0.13815044, Val Loss: 2.55397028 (Best Model Saved)\n",
      "Epoch 50/500 - Train Loss: 0.12768524, Val Loss: 2.53050196 (Best Model Saved)\n",
      "Epoch 51/500 - Train Loss: 0.13165747, Val Loss: 2.52352667 (Best Model Saved)\n",
      "Epoch 52/500 - Train Loss: 0.13136369, Val Loss: 2.50951749 (Best Model Saved)\n",
      "Epoch 53/500 - Train Loss: 0.11910919, Val Loss: 2.42855048 (Best Model Saved)\n",
      "Epoch 54/500 - Train Loss: 0.12028306, Val Loss: 2.44610822\n",
      "Epoch 55/500 - Train Loss: 0.12211736, Val Loss: 2.38007724 (Best Model Saved)\n",
      "Epoch 56/500 - Train Loss: 0.12916178, Val Loss: 2.33225977 (Best Model Saved)\n",
      "Epoch 57/500 - Train Loss: 0.13220202, Val Loss: 2.30195212 (Best Model Saved)\n",
      "Epoch 58/500 - Train Loss: 0.12182214, Val Loss: 2.31114936\n",
      "Epoch 59/500 - Train Loss: 0.13481241, Val Loss: 2.30143374 (Best Model Saved)\n",
      "Epoch 60/500 - Train Loss: 0.14038593, Val Loss: 2.19077727 (Best Model Saved)\n",
      "Epoch 61/500 - Train Loss: 0.12085241, Val Loss: 2.19931334\n",
      "Epoch 62/500 - Train Loss: 0.13285960, Val Loss: 2.18404049 (Best Model Saved)\n",
      "Epoch 63/500 - Train Loss: 0.11892574, Val Loss: 2.10597840 (Best Model Saved)\n",
      "Epoch 64/500 - Train Loss: 0.11630898, Val Loss: 2.10030994 (Best Model Saved)\n",
      "Epoch 65/500 - Train Loss: 0.12176915, Val Loss: 2.06278229 (Best Model Saved)\n",
      "Epoch 66/500 - Train Loss: 0.11036033, Val Loss: 2.04548317 (Best Model Saved)\n",
      "Epoch 67/500 - Train Loss: 0.11673595, Val Loss: 1.98666969 (Best Model Saved)\n",
      "Epoch 68/500 - Train Loss: 0.12894956, Val Loss: 1.98330942 (Best Model Saved)\n",
      "Epoch 69/500 - Train Loss: 0.11859539, Val Loss: 1.95652995 (Best Model Saved)\n",
      "Epoch 70/500 - Train Loss: 0.12148916, Val Loss: 1.93451130 (Best Model Saved)\n",
      "Epoch 71/500 - Train Loss: 0.11027842, Val Loss: 1.92082524 (Best Model Saved)\n",
      "Epoch 72/500 - Train Loss: 0.13513710, Val Loss: 1.85336611 (Best Model Saved)\n",
      "Epoch 73/500 - Train Loss: 0.10687022, Val Loss: 1.85496947\n",
      "Epoch 74/500 - Train Loss: 0.11700441, Val Loss: 1.82818177 (Best Model Saved)\n",
      "Epoch 75/500 - Train Loss: 0.10615676, Val Loss: 1.77235153 (Best Model Saved)\n",
      "Epoch 76/500 - Train Loss: 0.11836709, Val Loss: 1.77010447 (Best Model Saved)\n",
      "Epoch 77/500 - Train Loss: 0.10959156, Val Loss: 1.79253834\n",
      "Epoch 78/500 - Train Loss: 0.10591318, Val Loss: 1.72584766 (Best Model Saved)\n",
      "Epoch 79/500 - Train Loss: 0.10171622, Val Loss: 1.74487320\n",
      "Epoch 80/500 - Train Loss: 0.11501042, Val Loss: 1.68324718 (Best Model Saved)\n",
      "Epoch 81/500 - Train Loss: 0.11255617, Val Loss: 1.63951898 (Best Model Saved)\n",
      "Epoch 82/500 - Train Loss: 0.11467233, Val Loss: 1.69451112\n",
      "Epoch 83/500 - Train Loss: 0.10687879, Val Loss: 1.64972180\n",
      "Epoch 84/500 - Train Loss: 0.11214663, Val Loss: 1.63416913 (Best Model Saved)\n",
      "Epoch 85/500 - Train Loss: 0.10342298, Val Loss: 1.61362031 (Best Model Saved)\n",
      "Epoch 86/500 - Train Loss: 0.10554907, Val Loss: 1.63057765\n",
      "Epoch 87/500 - Train Loss: 0.11185520, Val Loss: 1.60944283 (Best Model Saved)\n",
      "Epoch 88/500 - Train Loss: 0.10490956, Val Loss: 1.51706350 (Best Model Saved)\n",
      "Epoch 89/500 - Train Loss: 0.11128579, Val Loss: 1.59892932\n",
      "Epoch 90/500 - Train Loss: 0.11839701, Val Loss: 1.54419720\n",
      "Epoch 91/500 - Train Loss: 0.11182337, Val Loss: 1.57199225\n",
      "Epoch 92/500 - Train Loss: 0.11562984, Val Loss: 1.47637022 (Best Model Saved)\n",
      "Epoch 93/500 - Train Loss: 0.10222992, Val Loss: 1.53222126\n",
      "Epoch 94/500 - Train Loss: 0.11329094, Val Loss: 1.49332547\n",
      "Epoch 95/500 - Train Loss: 0.09220787, Val Loss: 1.48736921\n",
      "Epoch 96/500 - Train Loss: 0.11619563, Val Loss: 1.42057252 (Best Model Saved)\n",
      "Epoch 97/500 - Train Loss: 0.09842968, Val Loss: 1.43593246\n",
      "Epoch 98/500 - Train Loss: 0.10948895, Val Loss: 1.47676283\n",
      "Epoch 99/500 - Train Loss: 0.10677203, Val Loss: 1.43198290\n",
      "Epoch 100/500 - Train Loss: 0.11443594, Val Loss: 1.41132241 (Best Model Saved)\n",
      "Epoch 101/500 - Train Loss: 0.09934738, Val Loss: 1.43421689\n",
      "Epoch 102/500 - Train Loss: 0.09413930, Val Loss: 1.37346390 (Best Model Saved)\n",
      "Epoch 103/500 - Train Loss: 0.10842122, Val Loss: 1.37292939 (Best Model Saved)\n",
      "Epoch 104/500 - Train Loss: 0.09429826, Val Loss: 1.38755006\n",
      "Epoch 105/500 - Train Loss: 0.09686090, Val Loss: 1.35464841 (Best Model Saved)\n",
      "Epoch 106/500 - Train Loss: 0.12304096, Val Loss: 1.34543249 (Best Model Saved)\n",
      "Epoch 107/500 - Train Loss: 0.10001235, Val Loss: 1.39174041\n",
      "Epoch 108/500 - Train Loss: 0.09395778, Val Loss: 1.37538484\n",
      "Epoch 109/500 - Train Loss: 0.10786152, Val Loss: 1.35241947\n",
      "Epoch 110/500 - Train Loss: 0.10677448, Val Loss: 1.32344525 (Best Model Saved)\n",
      "Epoch 111/500 - Train Loss: 0.09994982, Val Loss: 1.29991925 (Best Model Saved)\n",
      "Epoch 112/500 - Train Loss: 0.10653294, Val Loss: 1.38177237\n",
      "Epoch 113/500 - Train Loss: 0.11740102, Val Loss: 1.18509048 (Best Model Saved)\n",
      "Epoch 114/500 - Train Loss: 0.08499387, Val Loss: 1.29563448\n",
      "Epoch 115/500 - Train Loss: 0.09631659, Val Loss: 1.26162495\n",
      "Epoch 116/500 - Train Loss: 0.11159168, Val Loss: 1.29580289\n",
      "Epoch 117/500 - Train Loss: 0.10061857, Val Loss: 1.28305928\n",
      "Epoch 118/500 - Train Loss: 0.11079635, Val Loss: 1.26985258\n",
      "Epoch 119/500 - Train Loss: 0.10269814, Val Loss: 1.25947244\n",
      "Epoch 120/500 - Train Loss: 0.11152830, Val Loss: 1.22393364\n",
      "Epoch 121/500 - Train Loss: 0.10144668, Val Loss: 1.24166335\n",
      "Epoch 122/500 - Train Loss: 0.11272769, Val Loss: 1.13551173 (Best Model Saved)\n",
      "Epoch 123/500 - Train Loss: 0.09699864, Val Loss: 1.21782041\n",
      "Epoch 124/500 - Train Loss: 0.09784653, Val Loss: 1.16587794\n",
      "Epoch 125/500 - Train Loss: 0.13415274, Val Loss: 1.33985353\n",
      "Epoch 126/500 - Train Loss: 0.08677286, Val Loss: 1.16804987\n",
      "Epoch 127/500 - Train Loss: 0.09426505, Val Loss: 1.22096962\n",
      "Epoch 128/500 - Train Loss: 0.09135491, Val Loss: 1.19943826\n",
      "Epoch 129/500 - Train Loss: 0.10308627, Val Loss: 1.14770189\n",
      "Epoch 130/500 - Train Loss: 0.09954445, Val Loss: 1.23929799\n",
      "Epoch 131/500 - Train Loss: 0.09775534, Val Loss: 1.18359339\n",
      "Epoch 132/500 - Train Loss: 0.09930985, Val Loss: 1.18756419\n",
      "Epoch 133/500 - Train Loss: 0.10173067, Val Loss: 1.21709503\n",
      "Epoch 134/500 - Train Loss: 0.10385395, Val Loss: 1.18445435\n",
      "Epoch 135/500 - Train Loss: 0.09298206, Val Loss: 1.17884344\n",
      "Epoch 136/500 - Train Loss: 0.09344451, Val Loss: 1.22073928\n",
      "Epoch 137/500 - Train Loss: 0.09859501, Val Loss: 1.13720031\n",
      "Epoch 138/500 - Train Loss: 0.09689050, Val Loss: 1.20183888\n",
      "Epoch 139/500 - Train Loss: 0.11084743, Val Loss: 1.28936365\n",
      "Epoch 140/500 - Train Loss: 0.10269272, Val Loss: 1.13749416\n",
      "Epoch 141/500 - Train Loss: 0.10036304, Val Loss: 1.10392156 (Best Model Saved)\n",
      "Epoch 142/500 - Train Loss: 0.08659895, Val Loss: 1.12364435\n",
      "Epoch 143/500 - Train Loss: 0.11071418, Val Loss: 1.14282675\n",
      "Epoch 144/500 - Train Loss: 0.10076117, Val Loss: 1.16263108\n",
      "Epoch 145/500 - Train Loss: 0.10788571, Val Loss: 1.13144667\n",
      "Epoch 146/500 - Train Loss: 0.09737129, Val Loss: 1.09341410 (Best Model Saved)\n",
      "Epoch 147/500 - Train Loss: 0.10646649, Val Loss: 1.09014834 (Best Model Saved)\n",
      "Epoch 148/500 - Train Loss: 0.08992371, Val Loss: 1.08274844 (Best Model Saved)\n",
      "Epoch 149/500 - Train Loss: 0.10724549, Val Loss: 1.07364745 (Best Model Saved)\n",
      "Epoch 150/500 - Train Loss: 0.10357435, Val Loss: 1.11498441\n",
      "Epoch 151/500 - Train Loss: 0.10338052, Val Loss: 1.14416513\n",
      "Epoch 152/500 - Train Loss: 0.09129604, Val Loss: 1.09316610\n",
      "Epoch 153/500 - Train Loss: 0.09614300, Val Loss: 1.05381665 (Best Model Saved)\n",
      "Epoch 154/500 - Train Loss: 0.10233961, Val Loss: 1.01085922 (Best Model Saved)\n",
      "Epoch 155/500 - Train Loss: 0.09456880, Val Loss: 1.13015017\n",
      "Epoch 156/500 - Train Loss: 0.10498743, Val Loss: 1.06289054\n",
      "Epoch 157/500 - Train Loss: 0.10157717, Val Loss: 1.12337208\n",
      "Epoch 158/500 - Train Loss: 0.10003718, Val Loss: 1.05347353\n",
      "Epoch 159/500 - Train Loss: 0.10067588, Val Loss: 1.04356895\n",
      "Epoch 160/500 - Train Loss: 0.10986378, Val Loss: 1.08201629\n",
      "Epoch 161/500 - Train Loss: 0.11468631, Val Loss: 1.14306712\n",
      "Epoch 162/500 - Train Loss: 0.10927016, Val Loss: 1.05090296\n",
      "Epoch 163/500 - Train Loss: 0.08831987, Val Loss: 1.00603937 (Best Model Saved)\n",
      "Epoch 164/500 - Train Loss: 0.08830038, Val Loss: 1.07195258\n",
      "Epoch 165/500 - Train Loss: 0.09090847, Val Loss: 1.09519693\n",
      "Epoch 166/500 - Train Loss: 0.09654726, Val Loss: 1.07857585\n",
      "Epoch 167/500 - Train Loss: 0.10821683, Val Loss: 1.12487195\n",
      "Epoch 168/500 - Train Loss: 0.08891562, Val Loss: 1.01337352\n",
      "Epoch 169/500 - Train Loss: 0.09695636, Val Loss: 1.06418870\n",
      "Epoch 170/500 - Train Loss: 0.09732013, Val Loss: 1.01910527\n",
      "Epoch 171/500 - Train Loss: 0.10216343, Val Loss: 1.10419111\n",
      "Epoch 172/500 - Train Loss: 0.11386689, Val Loss: 1.03323555\n",
      "Epoch 173/500 - Train Loss: 0.10022923, Val Loss: 1.01901208\n",
      "Epoch 174/500 - Train Loss: 0.11533529, Val Loss: 1.09370606\n",
      "Epoch 175/500 - Train Loss: 0.10642234, Val Loss: 0.95837000 (Best Model Saved)\n",
      "Epoch 176/500 - Train Loss: 0.09369879, Val Loss: 1.08709893\n",
      "Epoch 177/500 - Train Loss: 0.09161884, Val Loss: 0.97569147\n",
      "Epoch 178/500 - Train Loss: 0.10262603, Val Loss: 1.04361671\n",
      "Epoch 179/500 - Train Loss: 0.10057875, Val Loss: 1.05422036\n",
      "Epoch 180/500 - Train Loss: 0.09976677, Val Loss: 0.97766566\n",
      "Epoch 181/500 - Train Loss: 0.09500653, Val Loss: 1.04683517\n",
      "Epoch 182/500 - Train Loss: 0.08971442, Val Loss: 0.99827766\n",
      "Epoch 183/500 - Train Loss: 0.09042774, Val Loss: 0.98571815\n",
      "Epoch 184/500 - Train Loss: 0.09721011, Val Loss: 1.01998025\n",
      "Epoch 185/500 - Train Loss: 0.10399079, Val Loss: 0.97776727\n",
      "Epoch 186/500 - Train Loss: 0.11206157, Val Loss: 0.95991535\n",
      "Epoch 187/500 - Train Loss: 0.11065042, Val Loss: 1.00976342\n",
      "Epoch 188/500 - Train Loss: 0.08619037, Val Loss: 0.93360797 (Best Model Saved)\n",
      "Epoch 189/500 - Train Loss: 0.09263618, Val Loss: 1.00302058\n",
      "Epoch 190/500 - Train Loss: 0.09178684, Val Loss: 1.05126691\n",
      "Epoch 191/500 - Train Loss: 0.11094018, Val Loss: 1.04021476\n",
      "Epoch 192/500 - Train Loss: 0.08811061, Val Loss: 0.98466998\n",
      "Epoch 193/500 - Train Loss: 0.08601802, Val Loss: 0.98478225\n",
      "Epoch 194/500 - Train Loss: 0.09511738, Val Loss: 0.96565491\n",
      "Epoch 195/500 - Train Loss: 0.08973661, Val Loss: 0.99738209\n",
      "Epoch 196/500 - Train Loss: 0.10066366, Val Loss: 0.97009329\n",
      "Epoch 197/500 - Train Loss: 0.08177183, Val Loss: 0.96247980\n",
      "Epoch 198/500 - Train Loss: 0.09690323, Val Loss: 1.00930206\n",
      "Epoch 199/500 - Train Loss: 0.10682278, Val Loss: 1.09409636\n",
      "Epoch 200/500 - Train Loss: 0.09492819, Val Loss: 0.99303438\n",
      "Epoch 201/500 - Train Loss: 0.10404199, Val Loss: 1.08404040\n",
      "Epoch 202/500 - Train Loss: 0.09454784, Val Loss: 1.00900833\n",
      "Epoch 203/500 - Train Loss: 0.10300926, Val Loss: 1.01569302\n",
      "Epoch 204/500 - Train Loss: 0.08645203, Val Loss: 1.03155394\n",
      "Epoch 205/500 - Train Loss: 0.12554502, Val Loss: 0.97636347\n",
      "Epoch 206/500 - Train Loss: 0.08802994, Val Loss: 0.97732297\n",
      "Epoch 207/500 - Train Loss: 0.11627458, Val Loss: 0.96681352\n",
      "Epoch 208/500 - Train Loss: 0.09355511, Val Loss: 1.07511045\n",
      "Epoch 209/500 - Train Loss: 0.09581264, Val Loss: 0.95803496\n",
      "Epoch 210/500 - Train Loss: 0.08523004, Val Loss: 1.09406462\n",
      "Epoch 211/500 - Train Loss: 0.10175562, Val Loss: 0.94895643\n",
      "Epoch 212/500 - Train Loss: 0.10336105, Val Loss: 1.05546434\n",
      "Epoch 213/500 - Train Loss: 0.08434016, Val Loss: 1.00660568\n",
      "Epoch 214/500 - Train Loss: 0.09649781, Val Loss: 1.04812638\n",
      "Epoch 215/500 - Train Loss: 0.11084976, Val Loss: 1.10375421\n",
      "Epoch 216/500 - Train Loss: 0.08944204, Val Loss: 0.93245722 (Best Model Saved)\n",
      "Epoch 217/500 - Train Loss: 0.11030456, Val Loss: 1.05434518\n",
      "Epoch 218/500 - Train Loss: 0.08987962, Val Loss: 0.96945167\n",
      "Epoch 219/500 - Train Loss: 0.08235150, Val Loss: 1.00666527\n",
      "Epoch 220/500 - Train Loss: 0.08664922, Val Loss: 1.05072908\n",
      "Epoch 221/500 - Train Loss: 0.09288698, Val Loss: 0.97420995\n",
      "Epoch 222/500 - Train Loss: 0.09805924, Val Loss: 0.98564477\n",
      "Epoch 223/500 - Train Loss: 0.10861152, Val Loss: 0.97580694\n",
      "Epoch 224/500 - Train Loss: 0.10226014, Val Loss: 0.94652660\n",
      "Epoch 225/500 - Train Loss: 0.10556357, Val Loss: 0.99252902\n",
      "Epoch 226/500 - Train Loss: 0.07440220, Val Loss: 0.96272342\n",
      "Epoch 227/500 - Train Loss: 0.11677213, Val Loss: 1.15225373\n",
      "Epoch 228/500 - Train Loss: 0.09282113, Val Loss: 0.94597197\n",
      "Epoch 229/500 - Train Loss: 0.08552734, Val Loss: 1.00537628\n",
      "Epoch 230/500 - Train Loss: 0.08454785, Val Loss: 0.94669126\n",
      "Epoch 231/500 - Train Loss: 0.09441853, Val Loss: 1.01443754\n",
      "Epoch 232/500 - Train Loss: 0.09241959, Val Loss: 0.98947901\n",
      "Epoch 233/500 - Train Loss: 0.10275642, Val Loss: 1.01067007\n",
      "Epoch 234/500 - Train Loss: 0.09848643, Val Loss: 1.05880743\n",
      "Epoch 235/500 - Train Loss: 0.09088845, Val Loss: 1.02655135\n",
      "Epoch 236/500 - Train Loss: 0.10571835, Val Loss: 1.01809841\n",
      "Epoch 237/500 - Train Loss: 0.11038599, Val Loss: 0.95998949\n",
      "Epoch 238/500 - Train Loss: 0.09431597, Val Loss: 0.95072563\n",
      "Epoch 239/500 - Train Loss: 0.09045395, Val Loss: 1.02712454\n",
      "Epoch 240/500 - Train Loss: 0.08367672, Val Loss: 0.97492102\n",
      "Epoch 241/500 - Train Loss: 0.10287104, Val Loss: 1.04471852\n",
      "Epoch 242/500 - Train Loss: 0.09788021, Val Loss: 0.96706088\n",
      "Epoch 243/500 - Train Loss: 0.08635664, Val Loss: 0.95732269\n",
      "Epoch 244/500 - Train Loss: 0.10363292, Val Loss: 0.99403812\n",
      "Epoch 245/500 - Train Loss: 0.07935990, Val Loss: 1.00686209\n",
      "Epoch 246/500 - Train Loss: 0.09394778, Val Loss: 1.01883836\n",
      "Epoch 247/500 - Train Loss: 0.08776719, Val Loss: 0.98092790\n",
      "Epoch 248/500 - Train Loss: 0.09092565, Val Loss: 0.95330428\n",
      "Epoch 249/500 - Train Loss: 0.09870914, Val Loss: 1.00896460\n",
      "Epoch 250/500 - Train Loss: 0.09462911, Val Loss: 0.93336308\n",
      "Epoch 251/500 - Train Loss: 0.08483044, Val Loss: 0.98979907\n",
      "Epoch 252/500 - Train Loss: 0.09633319, Val Loss: 0.97171745\n",
      "Epoch 253/500 - Train Loss: 0.08592787, Val Loss: 1.00445767\n",
      "Epoch 254/500 - Train Loss: 0.09444905, Val Loss: 0.94030540\n",
      "Epoch 255/500 - Train Loss: 0.10034326, Val Loss: 1.04422009\n",
      "Epoch 256/500 - Train Loss: 0.10024856, Val Loss: 0.99120484\n",
      "Epoch 257/500 - Train Loss: 0.09789039, Val Loss: 0.99622209\n",
      "Epoch 258/500 - Train Loss: 0.10262509, Val Loss: 0.96764025\n",
      "Epoch 259/500 - Train Loss: 0.10553588, Val Loss: 1.09242584\n",
      "Epoch 260/500 - Train Loss: 0.09148740, Val Loss: 0.95737480\n",
      "Epoch 261/500 - Train Loss: 0.09856818, Val Loss: 1.03331336\n",
      "Epoch 262/500 - Train Loss: 0.08766075, Val Loss: 0.97134355\n",
      "Epoch 263/500 - Train Loss: 0.10742925, Val Loss: 1.01598799\n",
      "Epoch 264/500 - Train Loss: 0.09737729, Val Loss: 0.90807422 (Best Model Saved)\n",
      "Epoch 265/500 - Train Loss: 0.09603217, Val Loss: 0.97732712\n",
      "Epoch 266/500 - Train Loss: 0.09876055, Val Loss: 0.98599194\n",
      "Epoch 267/500 - Train Loss: 0.09585869, Val Loss: 0.98263153\n",
      "Epoch 268/500 - Train Loss: 0.09989110, Val Loss: 0.94833280\n",
      "Epoch 269/500 - Train Loss: 0.10866733, Val Loss: 0.98821850\n",
      "Epoch 270/500 - Train Loss: 0.09428183, Val Loss: 1.01312852\n",
      "Epoch 271/500 - Train Loss: 0.09198589, Val Loss: 0.90225276 (Best Model Saved)\n",
      "Epoch 272/500 - Train Loss: 0.08736649, Val Loss: 1.03903292\n",
      "Epoch 273/500 - Train Loss: 0.09658627, Val Loss: 0.90765683\n",
      "Epoch 274/500 - Train Loss: 0.08782021, Val Loss: 0.93655989\n",
      "Epoch 275/500 - Train Loss: 0.08973830, Val Loss: 0.95388952\n",
      "Epoch 276/500 - Train Loss: 0.10005946, Val Loss: 0.90787041\n",
      "Epoch 277/500 - Train Loss: 0.09160733, Val Loss: 0.98520836\n",
      "Epoch 278/500 - Train Loss: 0.09203175, Val Loss: 0.96932374\n",
      "Epoch 279/500 - Train Loss: 0.08651483, Val Loss: 0.98448044\n",
      "Epoch 280/500 - Train Loss: 0.10344001, Val Loss: 0.94465002\n",
      "Epoch 281/500 - Train Loss: 0.08906849, Val Loss: 0.96940850\n",
      "Epoch 282/500 - Train Loss: 0.10202351, Val Loss: 0.94963318\n",
      "Epoch 283/500 - Train Loss: 0.09809470, Val Loss: 0.94629332\n",
      "Epoch 284/500 - Train Loss: 0.09375574, Val Loss: 0.97880030\n",
      "Epoch 285/500 - Train Loss: 0.09873670, Val Loss: 0.96661466\n",
      "Epoch 286/500 - Train Loss: 0.10102016, Val Loss: 0.93870634\n",
      "Epoch 287/500 - Train Loss: 0.07718555, Val Loss: 0.96372752\n",
      "Epoch 288/500 - Train Loss: 0.09719970, Val Loss: 0.96676716\n",
      "Epoch 289/500 - Train Loss: 0.08762424, Val Loss: 0.93878341\n",
      "Epoch 290/500 - Train Loss: 0.09502428, Val Loss: 0.92049780\n",
      "Epoch 291/500 - Train Loss: 0.09902768, Val Loss: 0.97108480\n",
      "Epoch 292/500 - Train Loss: 0.09617914, Val Loss: 0.94503696\n",
      "Epoch 293/500 - Train Loss: 0.10071636, Val Loss: 0.94192868\n",
      "Epoch 294/500 - Train Loss: 0.09511881, Val Loss: 0.87315099 (Best Model Saved)\n",
      "Epoch 295/500 - Train Loss: 0.09552376, Val Loss: 0.95656312\n",
      "Epoch 296/500 - Train Loss: 0.07983162, Val Loss: 0.94887818\n",
      "Epoch 297/500 - Train Loss: 0.09143587, Val Loss: 1.01485530\n",
      "Epoch 298/500 - Train Loss: 0.10389726, Val Loss: 0.95071156\n",
      "Epoch 299/500 - Train Loss: 0.09910074, Val Loss: 0.98756564\n",
      "Epoch 300/500 - Train Loss: 0.09378275, Val Loss: 0.92126457\n",
      "Epoch 301/500 - Train Loss: 0.09902605, Val Loss: 0.97646028\n",
      "Epoch 302/500 - Train Loss: 0.09018496, Val Loss: 1.02216996\n",
      "Epoch 303/500 - Train Loss: 0.09494243, Val Loss: 0.96390666\n",
      "Epoch 304/500 - Train Loss: 0.07610797, Val Loss: 1.01871814\n",
      "Epoch 305/500 - Train Loss: 0.08666203, Val Loss: 0.93085797\n",
      "Epoch 306/500 - Train Loss: 0.08813021, Val Loss: 0.93784665\n",
      "Epoch 307/500 - Train Loss: 0.08580254, Val Loss: 0.95487723\n",
      "Epoch 308/500 - Train Loss: 0.09208354, Val Loss: 0.95954640\n",
      "Epoch 309/500 - Train Loss: 0.09964360, Val Loss: 0.90979779\n",
      "Epoch 310/500 - Train Loss: 0.11779050, Val Loss: 0.87779203\n",
      "Epoch 311/500 - Train Loss: 0.09309730, Val Loss: 0.94886015\n",
      "Epoch 312/500 - Train Loss: 0.09924427, Val Loss: 1.03822173\n",
      "Epoch 313/500 - Train Loss: 0.09846585, Val Loss: 0.95027572\n",
      "Epoch 314/500 - Train Loss: 0.09680916, Val Loss: 1.00024343\n",
      "Epoch 315/500 - Train Loss: 0.11690014, Val Loss: 1.03116003\n",
      "Epoch 316/500 - Train Loss: 0.07984419, Val Loss: 0.97822425\n",
      "Epoch 317/500 - Train Loss: 0.10083122, Val Loss: 1.03044546\n",
      "Epoch 318/500 - Train Loss: 0.09661239, Val Loss: 0.94337623\n",
      "Epoch 319/500 - Train Loss: 0.10253626, Val Loss: 1.03010391\n",
      "Epoch 320/500 - Train Loss: 0.09941036, Val Loss: 0.93968944\n",
      "Epoch 321/500 - Train Loss: 0.09064563, Val Loss: 0.93373467\n",
      "Epoch 322/500 - Train Loss: 0.11776672, Val Loss: 1.02497596\n",
      "Epoch 323/500 - Train Loss: 0.08870490, Val Loss: 0.97495057\n",
      "Epoch 324/500 - Train Loss: 0.07978481, Val Loss: 1.01594748\n",
      "Epoch 325/500 - Train Loss: 0.09897554, Val Loss: 0.96794882\n",
      "Epoch 326/500 - Train Loss: 0.08886627, Val Loss: 0.91487117\n",
      "Epoch 327/500 - Train Loss: 0.09966432, Val Loss: 0.96311033\n",
      "Epoch 328/500 - Train Loss: 0.10477057, Val Loss: 1.04545960\n",
      "Epoch 329/500 - Train Loss: 0.09832640, Val Loss: 0.91255733\n",
      "Epoch 330/500 - Train Loss: 0.08299718, Val Loss: 1.01778661\n",
      "Epoch 331/500 - Train Loss: 0.09037806, Val Loss: 0.98932020\n",
      "Epoch 332/500 - Train Loss: 0.09449849, Val Loss: 0.94482884\n",
      "Epoch 333/500 - Train Loss: 0.09856019, Val Loss: 0.94870670\n",
      "Epoch 334/500 - Train Loss: 0.08752872, Val Loss: 0.95269595\n",
      "Epoch 335/500 - Train Loss: 0.10247218, Val Loss: 0.90743054\n",
      "Epoch 336/500 - Train Loss: 0.08281599, Val Loss: 1.01078001\n",
      "Epoch 337/500 - Train Loss: 0.10052161, Val Loss: 1.01924983\n",
      "Epoch 338/500 - Train Loss: 0.09878056, Val Loss: 0.96428178\n",
      "Epoch 339/500 - Train Loss: 0.10054962, Val Loss: 0.98100126\n",
      "Epoch 340/500 - Train Loss: 0.08296407, Val Loss: 0.96609248\n",
      "Epoch 341/500 - Train Loss: 0.10410482, Val Loss: 1.00986712\n",
      "Epoch 342/500 - Train Loss: 0.09397267, Val Loss: 0.95806764\n",
      "Epoch 343/500 - Train Loss: 0.08504432, Val Loss: 0.94993778\n",
      "Epoch 344/500 - Train Loss: 0.08941059, Val Loss: 0.94358981\n",
      "Epoch 345/500 - Train Loss: 0.08291487, Val Loss: 0.97195019\n",
      "Epoch 346/500 - Train Loss: 0.10531050, Val Loss: 0.97808309\n",
      "Epoch 347/500 - Train Loss: 0.08928176, Val Loss: 0.98601188\n",
      "Epoch 348/500 - Train Loss: 0.08837227, Val Loss: 1.00034469\n",
      "Epoch 349/500 - Train Loss: 0.08270607, Val Loss: 0.98520234\n",
      "Epoch 350/500 - Train Loss: 0.09482412, Val Loss: 1.00222780\n",
      "Epoch 351/500 - Train Loss: 0.09412158, Val Loss: 0.91273737\n",
      "Epoch 352/500 - Train Loss: 0.09256695, Val Loss: 1.02809949\n",
      "Epoch 353/500 - Train Loss: 0.09405410, Val Loss: 0.91420732\n",
      "Epoch 354/500 - Train Loss: 0.09619783, Val Loss: 0.99217357\n",
      "Epoch 355/500 - Train Loss: 0.09243808, Val Loss: 0.97550049\n",
      "Epoch 356/500 - Train Loss: 0.10060577, Val Loss: 1.04200704\n",
      "Epoch 357/500 - Train Loss: 0.09767642, Val Loss: 0.95732692\n",
      "Epoch 358/500 - Train Loss: 0.09086828, Val Loss: 1.09177572\n",
      "Epoch 359/500 - Train Loss: 0.10152193, Val Loss: 0.94505756\n",
      "Epoch 360/500 - Train Loss: 0.10913903, Val Loss: 1.00068873\n",
      "Epoch 361/500 - Train Loss: 0.08723504, Val Loss: 0.98207252\n",
      "Epoch 362/500 - Train Loss: 0.08678532, Val Loss: 1.02103113\n",
      "Epoch 363/500 - Train Loss: 0.09275859, Val Loss: 0.99769154\n",
      "Epoch 364/500 - Train Loss: 0.08946719, Val Loss: 1.01185402\n",
      "Epoch 365/500 - Train Loss: 0.08466545, Val Loss: 1.00741221\n",
      "Epoch 366/500 - Train Loss: 0.09259867, Val Loss: 1.02789098\n",
      "Epoch 367/500 - Train Loss: 0.09963128, Val Loss: 0.96800289\n",
      "Epoch 368/500 - Train Loss: 0.09107109, Val Loss: 1.06068954\n",
      "Epoch 369/500 - Train Loss: 0.08708965, Val Loss: 0.99195138\n",
      "Epoch 370/500 - Train Loss: 0.10224602, Val Loss: 0.96509741\n",
      "Epoch 371/500 - Train Loss: 0.08260351, Val Loss: 1.00327314\n",
      "Epoch 372/500 - Train Loss: 0.09706345, Val Loss: 0.96253671\n",
      "Epoch 373/500 - Train Loss: 0.08561412, Val Loss: 0.98049243\n",
      "Epoch 374/500 - Train Loss: 0.09128723, Val Loss: 0.97428249\n",
      "Epoch 375/500 - Train Loss: 0.08615043, Val Loss: 1.02543299\n",
      "Epoch 376/500 - Train Loss: 0.09893945, Val Loss: 0.99194701\n",
      "Epoch 377/500 - Train Loss: 0.10368166, Val Loss: 1.02901554\n",
      "Epoch 378/500 - Train Loss: 0.09791744, Val Loss: 0.98863478\n",
      "Epoch 379/500 - Train Loss: 0.09782859, Val Loss: 1.00893196\n",
      "Epoch 380/500 - Train Loss: 0.09523164, Val Loss: 1.05428603\n",
      "Epoch 381/500 - Train Loss: 0.10057499, Val Loss: 0.93940821\n",
      "Epoch 382/500 - Train Loss: 0.07731667, Val Loss: 1.02660362\n",
      "Epoch 383/500 - Train Loss: 0.08501833, Val Loss: 1.00644121\n",
      "Epoch 384/500 - Train Loss: 0.10594809, Val Loss: 0.99961565\n",
      "Epoch 385/500 - Train Loss: 0.09570820, Val Loss: 0.98385155\n",
      "Epoch 386/500 - Train Loss: 0.09787779, Val Loss: 1.04265165\n",
      "Epoch 387/500 - Train Loss: 0.10332170, Val Loss: 0.96556959\n",
      "Epoch 388/500 - Train Loss: 0.09128948, Val Loss: 0.97553197\n",
      "Epoch 389/500 - Train Loss: 0.10008238, Val Loss: 1.01876250\n",
      "Epoch 390/500 - Train Loss: 0.09553368, Val Loss: 0.97923134\n",
      "Epoch 391/500 - Train Loss: 0.08881531, Val Loss: 1.02646028\n",
      "Epoch 392/500 - Train Loss: 0.09369037, Val Loss: 0.94166933\n",
      "Epoch 393/500 - Train Loss: 0.09225294, Val Loss: 0.97399223\n",
      "Epoch 394/500 - Train Loss: 0.08473389, Val Loss: 0.94335206\n",
      "Epoch 395/500 - Train Loss: 0.08253282, Val Loss: 0.96243817\n",
      "Epoch 396/500 - Train Loss: 0.09396749, Val Loss: 1.01740350\n",
      "Epoch 397/500 - Train Loss: 0.08528253, Val Loss: 0.93758370\n",
      "Epoch 398/500 - Train Loss: 0.11848726, Val Loss: 1.01353899\n",
      "Epoch 399/500 - Train Loss: 0.09417881, Val Loss: 1.00673102\n",
      "Epoch 400/500 - Train Loss: 0.09992033, Val Loss: 0.93332952\n",
      "Epoch 401/500 - Train Loss: 0.08463315, Val Loss: 0.99117011\n",
      "Epoch 402/500 - Train Loss: 0.09609020, Val Loss: 0.98086432\n",
      "Epoch 403/500 - Train Loss: 0.08097093, Val Loss: 1.03309309\n",
      "Epoch 404/500 - Train Loss: 0.10115723, Val Loss: 0.93619183\n",
      "Epoch 405/500 - Train Loss: 0.10209272, Val Loss: 0.97168675\n",
      "Epoch 406/500 - Train Loss: 0.07580860, Val Loss: 0.98530127\n",
      "Epoch 407/500 - Train Loss: 0.09315626, Val Loss: 1.02913924\n",
      "Epoch 408/500 - Train Loss: 0.08638634, Val Loss: 1.00217597\n",
      "Epoch 409/500 - Train Loss: 0.09602996, Val Loss: 1.01207569\n",
      "Epoch 410/500 - Train Loss: 0.10395583, Val Loss: 0.96502648\n",
      "Epoch 411/500 - Train Loss: 0.08879609, Val Loss: 1.02478693\n",
      "Epoch 412/500 - Train Loss: 0.09441382, Val Loss: 0.99720983\n",
      "Epoch 413/500 - Train Loss: 0.08545014, Val Loss: 1.00797388\n",
      "Epoch 414/500 - Train Loss: 0.10343440, Val Loss: 0.94547883\n",
      "Epoch 415/500 - Train Loss: 0.09889112, Val Loss: 1.05342090\n",
      "Epoch 416/500 - Train Loss: 0.08996695, Val Loss: 0.98561440\n",
      "Epoch 417/500 - Train Loss: 0.10108752, Val Loss: 1.02896644\n",
      "Epoch 418/500 - Train Loss: 0.09377958, Val Loss: 1.00657856\n",
      "Epoch 419/500 - Train Loss: 0.10064589, Val Loss: 1.05036005\n",
      "Epoch 420/500 - Train Loss: 0.09698020, Val Loss: 1.03753506\n",
      "Epoch 421/500 - Train Loss: 0.09822504, Val Loss: 1.03638461\n",
      "Epoch 422/500 - Train Loss: 0.10133915, Val Loss: 0.99048330\n",
      "Epoch 423/500 - Train Loss: 0.09816681, Val Loss: 1.04032059\n",
      "Epoch 424/500 - Train Loss: 0.08344342, Val Loss: 0.96068549\n",
      "Epoch 425/500 - Train Loss: 0.08075348, Val Loss: 1.03472291\n",
      "Epoch 426/500 - Train Loss: 0.07487594, Val Loss: 1.01263510\n",
      "Epoch 427/500 - Train Loss: 0.09096765, Val Loss: 1.04631732\n",
      "Epoch 428/500 - Train Loss: 0.08584261, Val Loss: 1.03351535\n",
      "Epoch 429/500 - Train Loss: 0.12618430, Val Loss: 1.07732818\n",
      "Epoch 430/500 - Train Loss: 0.10816466, Val Loss: 0.98610142\n",
      "Epoch 431/500 - Train Loss: 0.08616897, Val Loss: 1.00283566\n",
      "Epoch 432/500 - Train Loss: 0.10166293, Val Loss: 1.01599006\n",
      "Epoch 433/500 - Train Loss: 0.10404920, Val Loss: 1.00300771\n",
      "Epoch 434/500 - Train Loss: 0.09032900, Val Loss: 1.02706264\n",
      "Epoch 435/500 - Train Loss: 0.09114379, Val Loss: 1.05563261\n",
      "Epoch 436/500 - Train Loss: 0.08841755, Val Loss: 1.00301053\n",
      "Epoch 437/500 - Train Loss: 0.09270425, Val Loss: 1.06813765\n",
      "Epoch 438/500 - Train Loss: 0.08823860, Val Loss: 0.97914027\n",
      "Epoch 439/500 - Train Loss: 0.07962420, Val Loss: 1.03342563\n",
      "Epoch 440/500 - Train Loss: 0.09881047, Val Loss: 1.02415027\n",
      "Epoch 441/500 - Train Loss: 0.08331183, Val Loss: 1.04478320\n",
      "Epoch 442/500 - Train Loss: 0.08804908, Val Loss: 0.98491880\n",
      "Epoch 443/500 - Train Loss: 0.10861050, Val Loss: 1.07377166\n",
      "Epoch 444/500 - Train Loss: 0.08156159, Val Loss: 0.98753643\n",
      "Epoch 445/500 - Train Loss: 0.09532695, Val Loss: 1.07183507\n",
      "Epoch 446/500 - Train Loss: 0.09928821, Val Loss: 1.01068306\n",
      "Epoch 447/500 - Train Loss: 0.09658340, Val Loss: 1.04274836\n",
      "Epoch 448/500 - Train Loss: 0.10295753, Val Loss: 1.10564101\n",
      "Epoch 449/500 - Train Loss: 0.08386649, Val Loss: 1.04168758\n",
      "Epoch 450/500 - Train Loss: 0.09100443, Val Loss: 0.95277764\n",
      "Epoch 451/500 - Train Loss: 0.09462331, Val Loss: 1.03223869\n",
      "Epoch 452/500 - Train Loss: 0.08017658, Val Loss: 0.99537422\n",
      "Epoch 453/500 - Train Loss: 0.09615979, Val Loss: 1.00487205\n",
      "Epoch 454/500 - Train Loss: 0.08657279, Val Loss: 1.04973371\n",
      "Epoch 455/500 - Train Loss: 0.08709290, Val Loss: 0.98722126\n",
      "Epoch 456/500 - Train Loss: 0.08798807, Val Loss: 0.99534287\n",
      "Epoch 457/500 - Train Loss: 0.09577397, Val Loss: 0.96675494\n",
      "Epoch 458/500 - Train Loss: 0.09488221, Val Loss: 1.04391335\n",
      "Epoch 459/500 - Train Loss: 0.10145318, Val Loss: 0.99608493\n",
      "Epoch 460/500 - Train Loss: 0.07872991, Val Loss: 0.99992116\n",
      "Epoch 461/500 - Train Loss: 0.08777442, Val Loss: 1.04077037\n",
      "Epoch 462/500 - Train Loss: 0.10805797, Val Loss: 0.93511777\n",
      "Epoch 463/500 - Train Loss: 0.10206477, Val Loss: 1.03960356\n",
      "Epoch 464/500 - Train Loss: 0.08814075, Val Loss: 0.93208201\n",
      "Epoch 465/500 - Train Loss: 0.08988073, Val Loss: 1.01481989\n",
      "Epoch 466/500 - Train Loss: 0.08364224, Val Loss: 1.02316707\n",
      "Epoch 467/500 - Train Loss: 0.10448199, Val Loss: 0.98662128\n",
      "Epoch 468/500 - Train Loss: 0.09452844, Val Loss: 1.03470363\n",
      "Epoch 469/500 - Train Loss: 0.09412458, Val Loss: 1.00610338\n",
      "Epoch 470/500 - Train Loss: 0.08667633, Val Loss: 1.01891086\n",
      "Epoch 471/500 - Train Loss: 0.10469951, Val Loss: 1.04241602\n",
      "Epoch 472/500 - Train Loss: 0.09079387, Val Loss: 1.01261126\n",
      "Epoch 473/500 - Train Loss: 0.08494131, Val Loss: 0.99331178\n",
      "Epoch 474/500 - Train Loss: 0.08460354, Val Loss: 1.04519828\n",
      "Epoch 475/500 - Train Loss: 0.08946710, Val Loss: 1.02827470\n",
      "Epoch 476/500 - Train Loss: 0.09787234, Val Loss: 1.02711493\n",
      "Epoch 477/500 - Train Loss: 0.09097016, Val Loss: 1.01916234\n",
      "Epoch 478/500 - Train Loss: 0.08724269, Val Loss: 0.97433223\n",
      "Epoch 479/500 - Train Loss: 0.08630362, Val Loss: 1.01689556\n",
      "Epoch 480/500 - Train Loss: 0.09936021, Val Loss: 1.00852339\n",
      "Epoch 481/500 - Train Loss: 0.09894512, Val Loss: 1.04210284\n",
      "Epoch 482/500 - Train Loss: 0.09458804, Val Loss: 1.06612545\n",
      "Epoch 483/500 - Train Loss: 0.09792029, Val Loss: 1.01022743\n",
      "Epoch 484/500 - Train Loss: 0.09523333, Val Loss: 1.01139258\n",
      "Epoch 485/500 - Train Loss: 0.09835897, Val Loss: 1.05267817\n",
      "Epoch 486/500 - Train Loss: 0.09028725, Val Loss: 1.00070557\n",
      "Epoch 487/500 - Train Loss: 0.09275318, Val Loss: 1.05288073\n",
      "Epoch 488/500 - Train Loss: 0.08880197, Val Loss: 1.02802241\n",
      "Epoch 489/500 - Train Loss: 0.09431230, Val Loss: 1.03279299\n",
      "Epoch 490/500 - Train Loss: 0.09216068, Val Loss: 0.93975970\n",
      "Epoch 491/500 - Train Loss: 0.09859075, Val Loss: 1.01313908\n",
      "Epoch 492/500 - Train Loss: 0.08310255, Val Loss: 0.99239381\n",
      "Epoch 493/500 - Train Loss: 0.08906718, Val Loss: 1.04834035\n",
      "Epoch 494/500 - Train Loss: 0.09168667, Val Loss: 1.04906569\n",
      "Epoch 495/500 - Train Loss: 0.07750487, Val Loss: 1.00306927\n",
      "Epoch 496/500 - Train Loss: 0.10090410, Val Loss: 0.97812679\n",
      "Epoch 497/500 - Train Loss: 0.09721142, Val Loss: 1.03731515\n",
      "Epoch 498/500 - Train Loss: 0.09265057, Val Loss: 0.96068931\n",
      "Epoch 499/500 - Train Loss: 0.09528754, Val Loss: 1.02480948\n",
      "Epoch 500/500 - Train Loss: 0.09345493, Val Loss: 1.02585433\n",
      "Loading overall best model from epoch 293\n",
      "\n",
      "Model Evaluation Metrics:\n",
      "MSE                 : 1.049119\n",
      "RMSE                : 1.024265\n",
      "MAE                 : 0.913702\n",
      "R2                  : -1.691798\n",
      "MAPE                : 16.213185\n",
      "EXPLAINED_VARIANCE  : 0.449078\n",
      "MAX_ERROR           : 2.084092\n",
      "\n",
      "Final lstm Model Performance:\n",
      "Validation RMSE: 1.02426504\n",
      "Validation MAE: 0.91370195\n",
      "Validation R²: -1.69179845\n",
      "Validation MAPE: 16.2132%\n",
      "\n",
      "Final model and results saved to Results/HPO\n",
      "Best model achieved at epoch 293\n",
      "Interactive visualizations saved as HTML files for better exploration\n",
      "\n",
      "Optimization complete!\n",
      "Best validation RMSE: 5.45622694\n",
      "Best parameters: {'hidden_size': 403, 'num_layers': 3, 'dropout': 0.36599697090570255, 'learning_rate': 0.0006251373574521745, 'weight_decay': 2.9380279387035354e-06, 'batch_norm': True, 'cell_dropout': 0.4330880728874676, 'optimizer': 'sgd', 'lr_scheduler': 'none', 'loss_function': 'mae'}\n"
     ]
    }
   ],
   "source": [
    "from Optimization.hyperparameter_optimization import run_hyperparameter_optimization\n",
    "data_file = train_data_ready  # Replace with your actual data file\n",
    "model_type = \"lstm\"\n",
    "n_trials = 2\n",
    "output_dir = \"Results/HPO\"\n",
    "\n",
    "# Print welcome message\n",
    "print(f\"Starting Bayesian HPO for {model_type.upper()} model with {n_trials} trials\")\n",
    "print(f\"Data file: {data_file}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "# Run hyperparameter optimization\n",
    "best_params, study = run_hyperparameter_optimization(\n",
    "    data_file, \n",
    "    output_dir=output_dir, \n",
    "    n_trials=n_trials, \n",
    "    model_type=model_type\n",
    ")\n",
    "\n",
    "print(\"\\nOptimization complete!\")\n",
    "print(f\"Best validation RMSE: {study.best_value:.8f}\")\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb580ed-397c-4278-9b9d-8e1aeb2d56bb",
   "metadata": {},
   "source": [
    "#### 10. Test Set Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4443c87e-9b1e-4178-ab6e-97262f5934d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Imports\n",
    "# ====================================\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Custom modules\n",
    "from Models.evaluate_model import evaluate_model\n",
    "from Optimization.select_features_for_model import select_features_for_model\n",
    "from PlotScripts.get_time_series_comparison_plot import time_series_comparison_plot\n",
    "from PlotScripts.get_scatter_plot import scatter_plot\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Step 1: Data Preparation\n",
    "# ====================================\n",
    "df = test_data_ready.copy()  # Work on a copy to preserve original\n",
    "valid_features = select_features_for_model(df, \"lstm\")  # Select features suitable for LSTM\n",
    "seq_length = 30  # Length of input sequences\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Step 2: Feature & Target Scaling\n",
    "# ====================================\n",
    "X_all = X_scaler.transform(df[valid_features].values)   # Feature scaling\n",
    "y_all = df[\"Close\"].values.reshape(-1, 1)               # Target variable\n",
    "y_all_scaled = y_scaler.transform(y_all)                # Target scaling\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Step 3: Create Sliding Windows (Sequence Generation)\n",
    "# ====================================\n",
    "X_windows = []  # Sequences of input features\n",
    "y_targets = []  # Corresponding next-day targets\n",
    "\n",
    "for end_ix in range(seq_length, len(X_all)):\n",
    "    start_ix = end_ix - seq_length\n",
    "    X_windows.append(X_all[start_ix:end_ix])\n",
    "    y_targets.append(y_all_scaled[end_ix])\n",
    "\n",
    "X_tensor = torch.tensor(X_windows, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_targets, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Step 4: DataLoader Creation\n",
    "# ====================================\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "loader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Step 5: Model Evaluation\n",
    "# ====================================\n",
    "metrics = evaluate_model(Trained_Model, data_loader=loader)\n",
    "\n",
    "# ====================================\n",
    "# Step 6: Model Inference for Plotting\n",
    "# ====================================\n",
    "Trained_Model.eval()  # Set model to evaluation mode\n",
    "X_tensor = X_tensor.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred_scaled = Trained_Model(X_tensor).view(-1, 1).cpu().numpy()\n",
    "\n",
    "# Inverse scale to get actual price values\n",
    "y_pred = y_scaler.inverse_transform(y_pred_scaled).flatten()\n",
    "y_true = y_all[seq_length:].flatten()\n",
    "dates = df.index[seq_length:]\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Step 7: Visualization\n",
    "# ====================================\n",
    "time_series_comparison_plot(\n",
    "    targets_original=y_true,\n",
    "    predictions_original=y_pred,\n",
    "    model_type=\"LSTM\",\n",
    "    output_dir=\"Results\",\n",
    "    phase=\"Testing\"\n",
    ")\n",
    "scatter_plot(\n",
    "    targets_original=y_true,\n",
    "    predictions_original=y_pred,\n",
    "    model_type=\"LSTM\",\n",
    "    output_dir=\"Results\",\n",
    "    phase=\"Testing\"\n",
    ")\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Step 8: Store and Print Results\n",
    "# ====================================\n",
    "results = {\n",
    "    f\"{ticker}_predictions\": y_pred,\n",
    "    f\"{ticker}_actuals\": y_true,\n",
    "    **{f\"{ticker}_{k}\": v for k, v in metrics.items()},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8aca96-7db9-4e6d-b878-841a9a821565",
   "metadata": {},
   "source": [
    "####  11. Generate Forecast "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6adec42e-b367-4a3d-a6a6-014943f76260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Close Price of INFY.NS for 2025-06-02: 1570.0154\n"
     ]
    }
   ],
   "source": [
    "# Import the forecasting function from the utility module\n",
    "from Utils.get_forecast import generate_forecast\n",
    "\n",
    "# Create a copy of the test data to avoid modifying the original dataset\n",
    "forecast_data = test_data_ready.copy()\n",
    "\n",
    "# Generate forecast using the trained LSTM model\n",
    "generate_forecast(\n",
    "    forecast_data=forecast_data,  # Input data prepared for forecasting\n",
    "    Trained_Model=Trained_Model,  # The trained model used for prediction\n",
    "    X_scaler=X_scaler,  # Scaler used to normalize input features\n",
    "    y_scaler=y_scaler,  # Scaler used to denormalize output predictions\n",
    "    seq_length=30,  # Number of time steps to look back for each prediction\n",
    "    ticker=ticker,  # Ticker symbol for the stock (or identifier for the time series)\n",
    "    model=\"lstm\",  # Type of model to be used (\"lstm\" in this case)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336bb91c-b1b0-47f5-b639-57d66ad291ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7779372-f0f8-423e-9805-387268fc7bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a141d1bd-9494-494e-8a62-ee899194235e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a267e12a-64c1-476c-9fbf-f4243bddd287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b533e5-275b-4b40-b3dc-105704b5cf31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535d65b6-2cd3-4907-91a2-1749ee2534de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
