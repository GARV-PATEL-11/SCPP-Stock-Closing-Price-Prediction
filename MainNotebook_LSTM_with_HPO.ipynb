{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a0c8d45-c5a3-493d-969a-120839816798",
   "metadata": {},
   "source": [
    "####  1. Select Stock Ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96720a6b-e4c8-4480-85ad-61438be2cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set the ticker symbol\n",
    "ticker = \"INFY.NS\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4a2103-01ea-49a4-a4e5-5303e5530c15",
   "metadata": {},
   "source": [
    "#### 2. Select appropriate device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816bb42a-e447-4405-963f-e4ee5f83e50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Select appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a47ee33-eef3-45f2-bff1-ac4a61514a41",
   "metadata": {},
   "source": [
    "#### 3. Run the CUDA check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c5d306-f730-439b-9aeb-41e36512fbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Config.check_cuda_config import check_cuda_configuration\n",
    "\n",
    "# Run the CUDA check\n",
    "check_cuda_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cb156e-1d9f-4a1b-ae36-95109d94b189",
   "metadata": {},
   "source": [
    "#### 4. Fetch stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c3bbb2-3af3-477a-8929-2e5cb3d379e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from DataPipeline.data_fetcher import fetch_daily_data_ist\n",
    "\n",
    "# Create the path if it doesn't exist\n",
    "raw_data_dir = \"Data/RawData\"\n",
    "\n",
    "# Fetch stock data\n",
    "data = fetch_daily_data_ist(ticker)\n",
    "\n",
    "# Construct filename and full path\n",
    "filename = f\"{ticker[:-3]}.csv\"\n",
    "filepath = os.path.join(raw_data_dir, filename)\n",
    "\n",
    "# Save to CSV\n",
    "data.to_csv(filepath)\n",
    "print(f\"Fetched raw data saved to: {filepath}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb345f3-45aa-4f53-9a3d-aba0096e8fce",
   "metadata": {},
   "source": [
    "#### 5. Calculate Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c463d7-703b-4952-a89e-72a7fcfb1062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataPipeline.technical_indicators import TechnicalIndicators\n",
    "\n",
    "# Calculate technical indicators\n",
    "indicators = TechnicalIndicators(data)\n",
    "indicators_data = indicators.calculate_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5232e794-c0ff-4b23-bd44-3b9f325ebe65",
   "metadata": {},
   "source": [
    "#### 6. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87703c18-5e35-4d98-9ffb-5ef3eb597394",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nProcessed data shape after indicators: {indicators_data.shape}\")\n",
    "\n",
    "# Split data into train and validation (85%) and test (15%)\n",
    "total_size = len(indicators_data)\n",
    "train_size = int(0.85 * total_size)\n",
    "\n",
    "train_data = indicators_data[:train_size].copy()\n",
    "test_data = indicators_data[train_size:].copy()\n",
    "\n",
    "print(f\"\\nData split sizes:\")\n",
    "print(f\"\\nTrain: {len(train_data)}, Test: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39821baf-ab1c-4f82-89a4-9f6a8a744bed",
   "metadata": {},
   "source": [
    "#### 7. Dropping NaN Columns and Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e72d01b-aabd-47d2-aebe-88119f5a4fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataPipeline.data_cleaning import drop_all_null_columns\n",
    "\n",
    "# Drop all-null columns\n",
    "train_data_ready, dropped_columns = drop_all_null_columns(train_data)\n",
    "test_data_ready, _ = drop_all_null_columns(test_data)\n",
    "\n",
    "print(f\"\\nDropped columns due to nulls: {dropped_columns}\")\n",
    "print(f\"Final shapes - Train: {train_data_ready.shape}, Test: {test_data_ready.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b08eca9-e706-4fc3-943a-cb771290ce88",
   "metadata": {},
   "source": [
    "#### 8. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc83a6a-426d-429b-8db6-7c5e34d0dd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Optimization.prepare_data_for_hpo import prepare_data_for_hpo\n",
    "\n",
    "# Prepare data for LSTM (only train loader, no val)\n",
    "data, train_loader, val_loader, X_scaler, y_scaler, feature_names, num_features = prepare_data_for_hpo(\n",
    "    Modelling_data=train_data_ready,\n",
    "    model_type=\"LSTM\",\n",
    "    batch_size=64,\n",
    "    seq_length=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e08e064-74c5-4171-9af8-4110adf93a7e",
   "metadata": {},
   "source": [
    "#### 9. Hyper Parameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557d1260-a1fa-431e-8497-18a3bd2e5d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Optimization.hyperparameter_optimization import run_hyperparameter_optimization\n",
    "data_file = train_data_ready  # Replace with your actual data file\n",
    "model_type = \"lstm\"\n",
    "n_trials = 2\n",
    "output_dir = \"Results/HPO\"\n",
    "\n",
    "# Print welcome message\n",
    "print(f\"Starting Bayesian HPO for {model_type.upper()} model with {n_trials} trials\")\n",
    "print(f\"Data file: {data_file}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "# Run hyperparameter optimization\n",
    "best_params, study = run_hyperparameter_optimization(\n",
    "    data_file, \n",
    "    output_dir=output_dir, \n",
    "    n_trials=n_trials, \n",
    "    model_type=model_type\n",
    ")\n",
    "\n",
    "print(\"\\nOptimization complete!\")\n",
    "print(f\"Best validation RMSE: {study.best_value:.8f}\")\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb580ed-397c-4278-9b9d-8e1aeb2d56bb",
   "metadata": {},
   "source": [
    "#### 10. Test Set Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4443c87e-9b1e-4178-ab6e-97262f5934d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Imports\n",
    "# ====================================\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Custom modules\n",
    "from Models.evaluate_model import evaluate_model\n",
    "from Optimization.select_features_for_model import select_features_for_model\n",
    "from PlotScripts.get_time_series_comparison_plot import time_series_comparison_plot\n",
    "from PlotScripts.get_scatter_plot import scatter_plot\n",
    "from PlotScripts.get_residual_analysis_plot import residual_analysis\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Step 1: Data Preparation\n",
    "# ====================================\n",
    "df = test_data_ready.copy()  # Work on a copy to preserve original\n",
    "valid_features = select_features_for_model(df, \"lstm\")  # Select features suitable for LSTM\n",
    "seq_length = 30  # Length of input sequences\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Step 2: Feature & Target Scaling\n",
    "# ====================================\n",
    "X_all = X_scaler.transform(df[valid_features].values)   # Feature scaling\n",
    "y_all = df[\"Close\"].values.reshape(-1, 1)               # Target variable\n",
    "y_all_scaled = y_scaler.transform(y_all)                # Target scaling\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Step 3: Create Sliding Windows (Sequence Generation)\n",
    "# ====================================\n",
    "X_windows = []  # Sequences of input features\n",
    "y_targets = []  # Corresponding next-day targets\n",
    "\n",
    "for end_ix in range(seq_length, len(X_all)):\n",
    "    start_ix = end_ix - seq_length\n",
    "    X_windows.append(X_all[start_ix:end_ix])\n",
    "    y_targets.append(y_all_scaled[end_ix])\n",
    "\n",
    "X_tensor = torch.tensor(X_windows, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_targets, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Step 4: DataLoader Creation\n",
    "# ====================================\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "loader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Step 5: Model Evaluation\n",
    "# ====================================\n",
    "metrics = evaluate_model(Trained_Model, data_loader=loader)\n",
    "\n",
    "# ====================================\n",
    "# Step 6: Model Inference for Plotting\n",
    "# ====================================\n",
    "Trained_Model.eval()  # Set model to evaluation mode\n",
    "X_tensor = X_tensor.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred_scaled = Trained_Model(X_tensor).view(-1, 1).cpu().numpy()\n",
    "\n",
    "# Inverse scale to get actual price values\n",
    "y_pred = y_scaler.inverse_transform(y_pred_scaled).flatten()\n",
    "y_true = y_all[seq_length:].flatten()\n",
    "dates = df.index[seq_length:]\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Step 7: Visualization\n",
    "# ====================================\n",
    "time_series_comparison_plot(\n",
    "    targets_original=y_true,\n",
    "    predictions_original=y_pred,\n",
    "    model_type=\"LSTM\",\n",
    "    output_dir=\"Results\",\n",
    "    phase=\"Testing\"\n",
    ")\n",
    "scatter_plot(\n",
    "    targets_original=y_true,\n",
    "    predictions_original=y_pred,\n",
    "    model_type=\"LSTM\",\n",
    "    output_dir=\"Results\",\n",
    "    phase=\"Testing\"\n",
    ")\n",
    "residual_analysis(\n",
    "    targets_original=y_true,\n",
    "    predictions_original=y_pred,\n",
    "    model_type=\"LSTM\",\n",
    "    output_dir=\"Results\",\n",
    "    phase=\"Testing\"\n",
    ")\n",
    "\n",
    "# ====================================\n",
    "# Step 8: Store and Print Results\n",
    "# ====================================\n",
    "results = {\n",
    "    f\"{ticker}_predictions\": y_pred,\n",
    "    f\"{ticker}_actuals\": y_true,\n",
    "    **{f\"{ticker}_{k}\": v for k, v in metrics.items()},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8aca96-7db9-4e6d-b878-841a9a821565",
   "metadata": {},
   "source": [
    "####  11. Generate Forecast "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adec42e-b367-4a3d-a6a6-014943f76260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the forecasting function from the utility module\n",
    "from Utils.get_forecast import generate_forecast\n",
    "\n",
    "# Create a copy of the test data to avoid modifying the original dataset\n",
    "forecast_data = test_data_ready.copy()\n",
    "\n",
    "# Generate forecast using the trained LSTM model\n",
    "generate_forecast(\n",
    "    forecast_data=forecast_data,  # Input data prepared for forecasting\n",
    "    Trained_Model=Trained_Model,  # The trained model used for prediction\n",
    "    X_scaler=X_scaler,  # Scaler used to normalize input features\n",
    "    y_scaler=y_scaler,  # Scaler used to denormalize output predictions\n",
    "    seq_length=30,  # Number of time steps to look back for each prediction\n",
    "    ticker=ticker,  # Ticker symbol for the stock (or identifier for the time series)\n",
    "    model=\"lstm\",  # Type of model to be used (\"lstm\" in this case)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336bb91c-b1b0-47f5-b639-57d66ad291ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7779372-f0f8-423e-9805-387268fc7bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a141d1bd-9494-494e-8a62-ee899194235e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a267e12a-64c1-476c-9fbf-f4243bddd287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b533e5-275b-4b40-b3dc-105704b5cf31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535d65b6-2cd3-4907-91a2-1749ee2534de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
